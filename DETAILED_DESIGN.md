# Cassandraç‰¹åŒ–å‹ã‚³ãƒ¼ãƒ‰åˆ†æã‚·ã‚¹ãƒ†ãƒ  - è©³ç´°è¨­è¨ˆæ›¸
## Phase 1 ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v2.0 (Detailed Design)  
**ä½œæˆæ—¥**: 2025å¹´10æœˆ26æ—¥  
**å¯¾è±¡èª­è€…**: å®Ÿè£…æ‹…å½“é–‹ç™ºè€…

---

## ğŸ“‹ ç›®æ¬¡

1. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°è¨­è¨ˆ](#1-ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°è¨­è¨ˆ)
2. [ã‚¯ãƒ©ã‚¹è¨­è¨ˆ](#2-ã‚¯ãƒ©ã‚¹è¨­è¨ˆ)
3. [ãƒ‡ãƒ¼ã‚¿æ§‹é€ è¨­è¨ˆ](#3-ãƒ‡ãƒ¼ã‚¿æ§‹é€ è¨­è¨ˆ)
4. [ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°](#4-ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°)
5. [ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆ](#5-ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆ)
6. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­è¨ˆ](#6-ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­è¨ˆ)
7. [ãƒ†ã‚¹ãƒˆè¨­è¨ˆ](#7-ãƒ†ã‚¹ãƒˆè¨­è¨ˆ)
8. [ãƒ­ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­è¨ˆ](#8-ãƒ­ã‚°ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­è¨ˆ)

---

## 1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°è¨­è¨ˆ

### 1.1 ãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Presentation Layer                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  CLI         â”‚  â”‚  HTML Report â”‚  â”‚  JSON Export â”‚      â”‚
â”‚  â”‚  Interface   â”‚  â”‚  Generator   â”‚  â”‚  API         â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Application Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         AnalysisOrchestrator                         â”‚   â”‚
â”‚  â”‚  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ç®¡ç†                                â”‚   â”‚
â”‚  â”‚  - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ                                    â”‚   â”‚
â”‚  â”‚  - çµæœé›†ç´„                                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Business Logic Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Parsers     â”‚  â”‚  Detectors   â”‚  â”‚  Analyzers   â”‚      â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚      â”‚
â”‚  â”‚ - Java       â”‚  â”‚ - ALLOW      â”‚  â”‚ - Impact     â”‚      â”‚
â”‚  â”‚ - CQL        â”‚  â”‚   FILTERING  â”‚  â”‚   Analysis   â”‚      â”‚
â”‚  â”‚ - AST        â”‚  â”‚ - Partition  â”‚  â”‚ - Metrics    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Access Layer                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  FileSystem  â”‚  â”‚  Config      â”‚  â”‚  Cache       â”‚      â”‚
â”‚  â”‚  Reader      â”‚  â”‚  Loader      â”‚  â”‚  Manager     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å‡¦ç†ãƒ•ãƒ­ãƒ¼è©³ç´°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
  [ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªèµ°æŸ»]              [ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°]
        â”‚                                 â”‚
        â”‚  - å†å¸°çš„æ¢ç´¢                   â”‚  - ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒ
        â”‚  - ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯è¿½è·¡       â”‚  - é™¤å¤–ãƒ«ãƒ¼ãƒ«é©ç”¨
        â”‚  - æ¨©é™ãƒã‚§ãƒƒã‚¯                 â”‚  - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™
        â”‚                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              [ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆç”Ÿæˆ]
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: ä¸¦åˆ—è§£æ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
  [ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ¼ãƒ«èµ·å‹•]            [ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼ç”Ÿæˆ]
        â”‚                                 â”‚
        â”‚  - ã‚¹ãƒ¬ãƒƒãƒ‰æ•°æ±ºå®š               â”‚  - ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²
        â”‚  - ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦               â”‚  - å„ªå…ˆåº¦ä»˜ã‘
        â”‚                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              [ä¸¦åˆ—è§£æå®Ÿè¡Œ]
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
    [Worker 1]      [Worker 2]      [Worker N]
        â”‚                â”‚                â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â””â”€â–¶â”‚  ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®è§£æ       â”‚â—€â”€â”˜
           â”‚  1. Javaæ§‹æ–‡è§£æ         â”‚
           â”‚  2. CQLæŠ½å‡º              â”‚
           â”‚  3. å•é¡Œæ¤œå‡º             â”‚
           â”‚  4. çµæœè¨˜éŒ²             â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: çµæœé›†ç´„                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
  [Issueé›†ç´„]                    [ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—]
        â”‚                                 â”‚
        â”‚  - é‡è¤‡é™¤å»                     â”‚  - é‡è¦åº¦åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
        â”‚  - å„ªå…ˆåº¦ã‚½ãƒ¼ãƒˆ                 â”‚  - ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥çµ±è¨ˆ
        â”‚  - ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–         â”‚  - å•é¡Œã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
        â”‚                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              [AnalysisResultç”Ÿæˆ]
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
        â–¼                                 â–¼
  [HTMLãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°]            [JSON/CSVå‡ºåŠ›]
        â”‚                                 â”‚
        â”‚  - Jinja2ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ           â”‚  - æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
        â”‚  - CSS/JSåŸ‹ã‚è¾¼ã¿               â”‚  - ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨
        â”‚  - ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¦ç´          â”‚
        â”‚                                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
              [ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿]
                         â†“
                    [å®Œäº†é€šçŸ¥]
```

### 1.3 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ç›¸äº’ä½œç”¨

```python
"""
ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³: å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æãƒ•ãƒ­ãƒ¼
"""

# 1. CLI â†’ AnalysisOrchestrator
CLI.execute(target_path)
    â†“
AnalysisOrchestrator.__init__()
    â†“
# 2. Orchestrator â†’ FileScanner
AnalysisOrchestrator.scan_files()
    â†“
FileScanner.scan_directory(target_path)
    â†’ [file1.java, file2.java, ...]
    â†“
# 3. Orchestrator â†’ AnalysisPipeline
for file in files:
    AnalysisPipeline.analyze_file(file)
        â†“
    # 4. Pipeline â†’ JavaParser
    JavaParser.parse_file(file)
        â†“
    # 4.1 æ§‹æ–‡è§£æ
    javalang.parse.parse(content)
        â†’ AST
        â†“
    # 4.2 Cassandraå‘¼ã³å‡ºã—æŠ½å‡º
    JavaParser._extract_cassandra_calls(AST)
        â†’ [CassandraCall1, CassandraCall2, ...]
        â†“
    # 5. Pipeline â†’ CQLParser
    for call in calls:
        CQLParser.analyze(call.cql_text)
            â†“
        # 5.1 CQLæ§‹æ–‡è§£æ
        CQLParser._tokenize(cql)
            â†’ tokens
            â†“
        # 5.2 å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        CQLParser._detect_patterns(tokens)
            â†’ CQLAnalysis
            â†“
    # 6. Pipeline â†’ Detectors
    for detector in detectors:
        detector.detect(call, cql_analysis)
            â†“
        # 6.1 ãƒ«ãƒ¼ãƒ«ãƒãƒƒãƒãƒ³ã‚°
        detector._match_rules(call, cql_analysis)
            â†“
        # 6.2 Issueç”Ÿæˆ
        if matched:
            Issue(...)
            â†“
    # 7. Pipeline â†’ ResultCollector
    ResultCollector.add_issues(issues)
    â†“
# 8. Orchestrator â†’ Reporter
AnalysisResult = ResultCollector.finalize()
Reporter.generate(AnalysisResult)
    â†“
# 9. Output
report.html
```

---

## 2. ã‚¯ãƒ©ã‚¹è¨­è¨ˆ

### 2.1 ã‚³ã‚¢ã‚¯ãƒ©ã‚¹å›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    <<interface>>                             â”‚
â”‚                    Parser                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + parse_file(path: Path): List[ParsedElement]              â”‚
â”‚ + get_supported_extensions(): List[str]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–³
                         â”‚ implements
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JavaParser     â”‚              â”‚ CQLParser      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - tree_walker  â”‚              â”‚ - tokenizer    â”‚
â”‚ - ast_cache    â”‚              â”‚ - grammar      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + parse_file() â”‚              â”‚ + analyze()    â”‚
â”‚ - _extract()   â”‚              â”‚ - _tokenize()  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    <<interface>>                             â”‚
â”‚                    Detector                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + detect(call, analysis): List[Issue]                       â”‚
â”‚ + get_detector_name(): str                                  â”‚
â”‚ + is_enabled(): bool                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â–³
                         â”‚ implements
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
â”‚AllowFiltering â”‚ â”‚PartitionKey â”‚ â”‚BatchSize   â”‚ â”‚Prepared  â”‚
â”‚Detector        â”‚ â”‚Detector     â”‚ â”‚Detector    â”‚ â”‚Statement â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚Detector  â”‚
â”‚- severity      â”‚ â”‚- schema_infoâ”‚ â”‚- threshold â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚- patterns      â”‚ â”‚- key_extrac.â”‚ â”‚- counter   â”‚ â”‚- min_execâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚+ detect()      â”‚ â”‚+ detect()   â”‚ â”‚+ detect()  â”‚ â”‚+ detect()â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 AnalysisOrchestrator                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ - config: Config                                            â”‚
â”‚ - file_scanner: FileScanner                                 â”‚
â”‚ - pipeline: AnalysisPipeline                                â”‚
â”‚ - result_collector: ResultCollector                         â”‚
â”‚ - thread_pool: ThreadPoolExecutor                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + __init__(config: Config)                                  â”‚
â”‚ + analyze(target: Path): AnalysisResult                     â”‚
â”‚ - _scan_files(target: Path): List[Path]                    â”‚
â”‚ - _analyze_parallel(files: List[Path]): List[Issue]        â”‚
â”‚ - _analyze_single_file(file: Path): List[Issue]            â”‚
â”‚ - _aggregate_results(issues: List[Issue]): AnalysisResult  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 è©³ç´°ã‚¯ãƒ©ã‚¹ä»•æ§˜

#### 2.2.1 JavaCassandraParser

```python
"""
JavaCassandraParser - Java ASTè§£æã¨Cassandraå‘¼ã³å‡ºã—æŠ½å‡º
"""

from dataclasses import dataclass, field
from typing import List, Optional, Dict, Set, Tuple
from pathlib import Path
import javalang
import re
import logging
from enum import Enum

logger = logging.getLogger(__name__)

class CallType(Enum):
    """Cassandraå‘¼ã³å‡ºã—ã®ã‚¿ã‚¤ãƒ—"""
    EXECUTE = "execute"
    EXECUTE_ASYNC = "executeAsync"
    PREPARE = "prepare"
    BATCH = "batch"
    UNKNOWN = "unknown"

@dataclass
class CassandraCall:
    """
    Cassandraå‘¼ã³å‡ºã—ã®è©³ç´°æƒ…å ±
    
    Attributes:
        call_type: å‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ—
        cql_text: CQLæ–‡å­—åˆ—ï¼ˆå®šæ•°å‚ç…§ã®å ´åˆã¯è§£æ±ºæ¸ˆã¿ï¼‰
        cql_is_constant: CQLãŒå®šæ•°ã‹ã©ã†ã‹
        constant_name: å®šæ•°åï¼ˆå®šæ•°ã®å ´åˆï¼‰
        line_number: è¡Œç•ªå·
        column_number: åˆ—ç•ªå·
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        class_name: ã‚¯ãƒ©ã‚¹å
        method_name: ãƒ¡ã‚½ãƒƒãƒ‰å
        is_prepared: Prepared Statementã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã‹
        consistency_level: æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«
        retry_policy: ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼
        timeout_ms: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆãƒŸãƒªç§’ï¼‰
        is_async: éåŒæœŸå®Ÿè¡Œã‹
        context: å‘¼ã³å‡ºã—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå‰å¾Œ3è¡Œï¼‰
    """
    call_type: CallType
    cql_text: str
    cql_is_constant: bool = False
    constant_name: Optional[str] = None
    line_number: int = 0
    column_number: int = 0
    file_path: str = ""
    class_name: str = ""
    method_name: str = ""
    is_prepared: bool = False
    consistency_level: Optional[str] = None
    retry_policy: Optional[str] = None
    timeout_ms: Optional[int] = None
    is_async: bool = False
    context: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """ãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        if not self.cql_text:
            raise ValueError("cql_text cannot be empty")
        
        if self.line_number < 0:
            raise ValueError("line_number must be non-negative")

class JavaCassandraParser:
    """
    Javaãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰Cassandraé–¢é€£ã®æ“ä½œã‚’æŠ½å‡º
    
    æ©Ÿèƒ½:
    1. Java ASTè§£æ
    2. Cassandra Sessionå‘¼ã³å‡ºã—ã®æ¤œå‡º
    3. CQLæ–‡å­—åˆ—ã®æŠ½å‡ºï¼ˆå®šæ•°è§£æ±ºå«ã‚€ï¼‰
    4. æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ã®æŠ½å‡º
    5. Prepared Statementåˆ¤å®š
    """
    
    # Cassandraã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    CASSANDRA_METHODS = {
        'execute', 'executeAsync', 'prepare', 
        'batch', 'prepareAsync'
    }
    
    # æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    CONSISTENCY_LEVEL_PATTERN = re.compile(
        r'ConsistencyLevel\.(ONE|TWO|THREE|QUORUM|ALL|LOCAL_QUORUM|'
        r'EACH_QUORUM|LOCAL_ONE|ANY|SERIAL|LOCAL_SERIAL)'
    )
    
    def __init__(self, config: Optional[Dict] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šè¾æ›¸
                - cache_enabled: ASTã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹åŒ– (default: True)
                - context_lines: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œæ•° (default: 3)
                - resolve_constants: å®šæ•°ã‚’è§£æ±ºã™ã‚‹ã‹ (default: True)
        """
        self.config = config or {}
        self._cache_enabled = self.config.get('cache_enabled', True)
        self._context_lines = self.config.get('context_lines', 3)
        self._resolve_constants = self.config.get('resolve_constants', True)
        
        # ASTã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ â†’ ASTï¼‰
        self._ast_cache: Dict[str, javalang.tree.CompilationUnit] = {}
        
        # å®šæ•°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å®šæ•°å®šç¾©ï¼‰
        self._constants_cache: Dict[str, Dict[str, str]] = {}
    
    def parse_file(self, file_path: Path) -> List[CassandraCall]:
        """
        Javaãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦Cassandraå‘¼ã³å‡ºã—ã‚’æŠ½å‡º
        
        Args:
            file_path: è§£æå¯¾è±¡ã®Javaãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            CassandraCallã®ãƒªã‚¹ãƒˆ
            
        Raises:
            FileNotFoundError: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„
            JavaSyntaxError: Javaæ§‹æ–‡ã‚¨ãƒ©ãƒ¼
        """
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        logger.info(f"Parsing file: {file_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            logger.warning(f"Failed to decode {file_path} as UTF-8, trying latin-1")
            with open(file_path, 'r', encoding='latin-1') as f:
                content = f.read()
        
        # ASTè§£æï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼‰
        file_hash = self._compute_hash(content)
        
        if self._cache_enabled and file_hash in self._ast_cache:
            logger.debug(f"Using cached AST for {file_path}")
            tree = self._ast_cache[file_hash]
        else:
            try:
                tree = javalang.parse.parse(content)
                if self._cache_enabled:
                    self._ast_cache[file_hash] = tree
            except javalang.parser.JavaSyntaxError as e:
                logger.error(f"Syntax error in {file_path}: {e}")
                raise
        
        # å®šæ•°ã®æŠ½å‡ºï¼ˆCQLå®šæ•°ã®è§£æ±ºã«ä½¿ç”¨ï¼‰
        if self._resolve_constants:
            constants = self._extract_constants(tree, content)
            self._constants_cache[str(file_path)] = constants
        else:
            constants = {}
        
        # Cassandraå‘¼ã³å‡ºã—ã®æŠ½å‡º
        calls = []
        lines = content.split('\n')
        
        # ã‚¯ãƒ©ã‚¹åã®å–å¾—
        class_name = self._get_class_name(tree)
        
        # MethodInvocationãƒãƒ¼ãƒ‰ã‚’èµ°æŸ»
        for path, node in tree.filter(javalang.tree.MethodInvocation):
            if self._is_cassandra_call(node):
                call = self._extract_call_info(
                    node, path, content, lines, 
                    file_path, class_name, constants
                )
                if call:
                    calls.append(call)
        
        logger.info(f"Found {len(calls)} Cassandra calls in {file_path}")
        return calls
    
    def _is_cassandra_call(self, node: javalang.tree.MethodInvocation) -> bool:
        """
        Cassandraé–¢é€£ã®å‘¼ã³å‡ºã—ã‹åˆ¤å®š
        
        åˆ¤å®šåŸºæº–:
        1. ãƒ¡ã‚½ãƒƒãƒ‰åãŒCassandraã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ã«ä¸€è‡´
        2. qualifierãŒsessionã¾ãŸã¯session-like
        
        Args:
            node: MethodInvocationãƒãƒ¼ãƒ‰
            
        Returns:
            Cassandraå‘¼ã³å‡ºã—ã®å ´åˆTrue
        """
        # ãƒ¡ã‚½ãƒƒãƒ‰åãƒã‚§ãƒƒã‚¯
        if node.member not in self.CASSANDRA_METHODS:
            return False
        
        # qualifierãƒã‚§ãƒƒã‚¯ï¼ˆsession, getSession(), etc.ï¼‰
        if hasattr(node, 'qualifier'):
            qualifier_str = str(node.qualifier)
            if 'session' in qualifier_str.lower():
                return True
        
        # qualifierãŒãªã„å ´åˆã‚‚ã‚ã‚‹ï¼ˆthis.sessionç­‰ï¼‰
        return True
    
    def _extract_call_info(
        self,
        node: javalang.tree.MethodInvocation,
        path: List,
        content: str,
        lines: List[str],
        file_path: Path,
        class_name: str,
        constants: Dict[str, str]
    ) -> Optional[CassandraCall]:
        """
        å‘¼ã³å‡ºã—æƒ…å ±ã®è©³ç´°æŠ½å‡º
        
        Args:
            node: MethodInvocationãƒãƒ¼ãƒ‰
            path: ASTãƒ‘ã‚¹
            content: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®å†…å®¹
            lines: ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œãƒªã‚¹ãƒˆ
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            class_name: ã‚¯ãƒ©ã‚¹å
            constants: å®šæ•°ãƒãƒƒãƒ—
            
        Returns:
            CassandraCallã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€æŠ½å‡ºå¤±æ•—æ™‚ã¯None
        """
        # è¡Œç•ªå·ãƒ»åˆ—ç•ªå·
        line_number = node.position.line if hasattr(node, 'position') else 0
        column_number = node.position.column if hasattr(node, 'position') else 0
        
        # CQLæ–‡å­—åˆ—ã®æŠ½å‡º
        cql_text, is_constant, constant_name = self._extract_cql_string(
            node, constants
        )
        
        if not cql_text:
            logger.debug(f"Could not extract CQL from {file_path}:{line_number}")
            return None
        
        # ãƒ¡ã‚½ãƒƒãƒ‰åã®å–å¾—ï¼ˆã©ã®ãƒ¡ã‚½ãƒƒãƒ‰å†…ã®å‘¼ã³å‡ºã—ã‹ï¼‰
        method_name = self._get_enclosing_method_name(path)
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡ºï¼ˆå‰å¾ŒNè¡Œï¼‰
        context = self._extract_context(lines, line_number, self._context_lines)
        
        # Prepared Statementåˆ¤å®š
        is_prepared = (
            node.member in {'prepare', 'prepareAsync'} or
            self._check_prepared_statement_usage(context)
        )
        
        # æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ã®æŠ½å‡º
        consistency_level = self._extract_consistency_level(context)
        
        # ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼ã®æŠ½å‡º
        retry_policy = self._extract_retry_policy(context)
        
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®æŠ½å‡º
        timeout_ms = self._extract_timeout(context)
        
        # å‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        call_type = self._determine_call_type(node.member)
        
        # éåŒæœŸåˆ¤å®š
        is_async = 'async' in node.member.lower()
        
        return CassandraCall(
            call_type=call_type,
            cql_text=cql_text,
            cql_is_constant=is_constant,
            constant_name=constant_name,
            line_number=line_number,
            column_number=column_number,
            file_path=str(file_path),
            class_name=class_name,
            method_name=method_name,
            is_prepared=is_prepared,
            consistency_level=consistency_level,
            retry_policy=retry_policy,
            timeout_ms=timeout_ms,
            is_async=is_async,
            context=context
        )
    
    def _extract_cql_string(
        self, 
        node: javalang.tree.MethodInvocation,
        constants: Dict[str, str]
    ) -> Tuple[Optional[str], bool, Optional[str]]:
        """
        ãƒ¡ã‚½ãƒƒãƒ‰å¼•æ•°ã‹ã‚‰CQLæ–‡å­—åˆ—ã‚’æŠ½å‡º
        
        æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
        1. æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«: "SELECT ..."
        2. æ–‡å­—åˆ—é€£çµ: "SELECT " + "* FROM users"
        3. å®šæ•°å‚ç…§: CQL_SELECT_USER
        4. StringBuilder: new StringBuilder().append("SELECT")...
        
        Args:
            node: MethodInvocationãƒãƒ¼ãƒ‰
            constants: å®šæ•°ãƒãƒƒãƒ—
            
        Returns:
            (CQLæ–‡å­—åˆ—, å®šæ•°ãƒ•ãƒ©ã‚°, å®šæ•°å)ã®ã‚¿ãƒ—ãƒ«
        """
        if not node.arguments:
            return None, False, None
        
        first_arg = node.arguments[0]
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«
        if isinstance(first_arg, javalang.tree.Literal):
            cql = first_arg.value.strip('"\'')
            return cql, False, None
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: å®šæ•°å‚ç…§
        if isinstance(first_arg, javalang.tree.MemberReference):
            constant_name = first_arg.member
            if constant_name in constants:
                cql = constants[constant_name]
                return cql, True, constant_name
            else:
                # å®šæ•°ãŒè§£æ±ºã§ããªã„å ´åˆã¯ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                return f"[CONSTANT: {constant_name}]", True, constant_name
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æ–‡å­—åˆ—é€£çµï¼ˆBinaryOperationï¼‰
        if isinstance(first_arg, javalang.tree.BinaryOperation):
            cql = self._resolve_binary_operation(first_arg, constants)
            return cql, False, None
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: StringBuilderï¼ˆå¾Œã§å®Ÿè£…ï¼‰
        # TODO: StringBuilder.append() ãƒã‚§ãƒ¼ãƒ³ã®è§£æ
        
        return None, False, None
    
    def _extract_constants(
        self, 
        tree: javalang.tree.CompilationUnit,
        content: str
    ) -> Dict[str, str]:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å®šæ•°å®šç¾©ã‚’æŠ½å‡º
        
        æŠ½å‡ºå¯¾è±¡:
        - public static final String CQL_XXX = "...";
        - private static final String QUERY_YYY = "...";
        
        Args:
            tree: AST
            content: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹
            
        Returns:
            å®šæ•°å â†’ å®šæ•°å€¤ã®ãƒãƒƒãƒ—
        """
        constants = {}
        
        for _, node in tree.filter(javalang.tree.FieldDeclaration):
            # static final ãƒã‚§ãƒƒã‚¯
            if not ('static' in node.modifiers and 'final' in node.modifiers):
                continue
            
            # Stringå‹ãƒã‚§ãƒƒã‚¯
            if node.type.name != 'String':
                continue
            
            # å€¤ã®æŠ½å‡º
            for declarator in node.declarators:
                if declarator.initializer:
                    if isinstance(declarator.initializer, javalang.tree.Literal):
                        value = declarator.initializer.value.strip('"\'')
                        constants[declarator.name] = value
        
        return constants
    
    def _check_prepared_statement_usage(self, context: List[str]) -> bool:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰Prepared Statementã®ä½¿ç”¨ã‚’åˆ¤å®š
        
        åˆ¤å®šãƒ‘ã‚¿ãƒ¼ãƒ³:
        - PreparedStatement ps = ...
        - BoundStatement bound = ...
        - session.prepare()ã®å‘¼ã³å‡ºã—
        
        Args:
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œ
            
        Returns:
            Prepared Statementä½¿ç”¨ã®å ´åˆTrue
        """
        context_str = '\n'.join(context)
        
        patterns = [
            r'PreparedStatement\s+\w+',
            r'BoundStatement\s+\w+',
            r'\.prepare\s*\(',
            r'\.bind\s*\('
        ]
        
        for pattern in patterns:
            if re.search(pattern, context_str):
                return True
        
        return False
    
    def _extract_consistency_level(self, context: List[str]) -> Optional[str]:
        """
        ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ã‚’æŠ½å‡º
        
        æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
        - .setConsistencyLevel(ConsistencyLevel.QUORUM)
        - ConsistencyLevel.ONE
        
        Args:
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œ
            
        Returns:
            æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«ï¼ˆä¾‹: "QUORUM"ï¼‰ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆNone
        """
        context_str = '\n'.join(context)
        
        match = self.CONSISTENCY_LEVEL_PATTERN.search(context_str)
        if match:
            return match.group(1)
        
        return None
    
    def _extract_retry_policy(self, context: List[str]) -> Optional[str]:
        """ãƒªãƒˆãƒ©ã‚¤ãƒãƒªã‚·ãƒ¼ã®æŠ½å‡º"""
        context_str = '\n'.join(context)
        
        patterns = [
            r'DefaultRetryPolicy',
            r'DowngradingConsistencyRetryPolicy',
            r'FallthroughRetryPolicy',
            r'LoggingRetryPolicy'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, context_str)
            if match:
                return match.group(0)
        
        return None
    
    def _extract_timeout(self, context: List[str]) -> Optional[int]:
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã®æŠ½å‡º"""
        context_str = '\n'.join(context)
        
        # setTimeout(5000) ã®ã‚ˆã†ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        match = re.search(r'setTimeout\s*\(\s*(\d+)\s*\)', context_str)
        if match:
            return int(match.group(1))
        
        return None
    
    def _extract_context(
        self, 
        lines: List[str], 
        line_number: int, 
        context_lines: int
    ) -> List[str]:
        """
        æŒ‡å®šè¡Œã®å‰å¾Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        
        Args:
            lines: ãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨è¡Œ
            line_number: å¯¾è±¡è¡Œç•ªå·ï¼ˆ1-indexedï¼‰
            context_lines: å‰å¾Œã®è¡Œæ•°
            
        Returns:
            ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¡Œã®ãƒªã‚¹ãƒˆ
        """
        start = max(0, line_number - context_lines - 1)
        end = min(len(lines), line_number + context_lines)
        return lines[start:end]
    
    def _get_class_name(self, tree: javalang.tree.CompilationUnit) -> str:
        """ASTã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’å–å¾—"""
        for _, node in tree.filter(javalang.tree.ClassDeclaration):
            return node.name
        return "Unknown"
    
    def _get_enclosing_method_name(self, path: List) -> str:
        """ASTãƒ‘ã‚¹ã‹ã‚‰åŒ…å«ãƒ¡ã‚½ãƒƒãƒ‰åã‚’å–å¾—"""
        for node in reversed(path):
            if isinstance(node, javalang.tree.MethodDeclaration):
                return node.name
        return "unknown"
    
    def _determine_call_type(self, method_name: str) -> CallType:
        """ãƒ¡ã‚½ãƒƒãƒ‰åã‹ã‚‰å‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        method_lower = method_name.lower()
        
        if 'prepare' in method_lower:
            return CallType.PREPARE
        elif 'batch' in method_lower:
            return CallType.BATCH
        elif 'async' in method_lower:
            return CallType.EXECUTE_ASYNC
        elif 'execute' in method_lower:
            return CallType.EXECUTE
        else:
            return CallType.UNKNOWN
    
    def _compute_hash(self, content: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
        import hashlib
        return hashlib.sha256(content.encode()).hexdigest()
    
    def _resolve_binary_operation(
        self,
        node: javalang.tree.BinaryOperation,
        constants: Dict[str, str]
    ) -> str:
        """
        æ–‡å­—åˆ—é€£çµï¼ˆBinaryOperationï¼‰ã‚’è§£æ±º
        
        ä¾‹: "SELECT " + "* FROM users" â†’ "SELECT * FROM users"
        
        Args:
            node: BinaryOperationãƒãƒ¼ãƒ‰
            constants: å®šæ•°ãƒãƒƒãƒ—
            
        Returns:
            é€£çµå¾Œã®æ–‡å­—åˆ—
        """
        # TODO: å†å¸°çš„ã«å·¦å³ã®ã‚ªãƒšãƒ©ãƒ³ãƒ‰ã‚’è§£æ±º
        # ç°¡æ˜“å®Ÿè£…: å·¦å³ãŒãƒªãƒ†ãƒ©ãƒ«ã®å ´åˆã®ã¿å¯¾å¿œ
        left = ""
        right = ""
        
        if isinstance(node.operandl, javalang.tree.Literal):
            left = node.operandl.value.strip('"\'')
        
        if isinstance(node.operandr, javalang.tree.Literal):
            right = node.operandr.value.strip('"\'')
        
        return left + right
```

#### 2.2.2 CQLParser

```python
"""
CQLParser - CQLæ–‡å­—åˆ—ã®è§£æã¨å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
"""

import re
from typing import List, Dict, Set, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

class QueryType(Enum):
    """ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—"""
    SELECT = "SELECT"
    INSERT = "INSERT"
    UPDATE = "UPDATE"
    DELETE = "DELETE"
    BATCH = "BATCH"
    CREATE_TABLE = "CREATE TABLE"
    ALTER_TABLE = "ALTER TABLE"
    DROP_TABLE = "DROP TABLE"
    CREATE_INDEX = "CREATE INDEX"
    TRUNCATE = "TRUNCATE"
    UNKNOWN = "UNKNOWN"

@dataclass
class WhereClause:
    """WHEREå¥ã®è©³ç´°æƒ…å ±"""
    raw_text: str
    conditions: List[Dict[str, any]] = field(default_factory=list)
    has_partition_key_filter: bool = False
    has_clustering_key_filter: bool = False
    uses_in_clause: bool = False
    uses_range_query: bool = False
    filter_columns: Set[str] = field(default_factory=set)

@dataclass
class CQLAnalysis:
    """
    CQLåˆ†æçµæœ
    
    Attributes:
        query_type: ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—
        tables: å¯¾è±¡ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒªã‚¹ãƒˆ
        has_allow_filtering: ALLOW FILTERINGã®ä½¿ç”¨æœ‰ç„¡
        where_clause: WHEREå¥ã®è©³ç´°
        is_batch: BATCHå‡¦ç†ã‹
        batch_size: BATCHå†…ã®ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆæ•°
        uses_prepared_statement_marker: ?ãƒãƒ¼ã‚«ãƒ¼ä½¿ç”¨æœ‰ç„¡
        select_columns: SELECTå¯¾è±¡ã‚«ãƒ©ãƒ 
        consistency_level_in_cql: CQLå†…ã®æ•´åˆæ€§ãƒ¬ãƒ™ãƒ«æŒ‡å®š
        limit_clause: LIMITå€¤
        order_by_clause: ORDER BYå¥
        issues: æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ
    """
    query_type: QueryType
    tables: List[str]
    has_allow_filtering: bool = False
    where_clause: Optional[WhereClause] = None
    is_batch: bool = False
    batch_size: int = 0
    uses_prepared_statement_marker: bool = False
    select_columns: List[str] = field(default_factory=list)
    consistency_level_in_cql: Optional[str] = None
    limit_clause: Optional[int] = None
    order_by_clause: Optional[str] = None
    issues: List[Dict] = field(default_factory=list)

class CQLParser:
    """
    CQLæ–‡ã‚’è§£æã—ã¦æ§‹é€ ã¨å•é¡Œã‚’æ¤œå‡º
    
    æ©Ÿèƒ½:
    1. CQLãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    2. ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
    3. WHEREå¥ã®è©³ç´°è§£æ
    4. å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
    """
    
    # CQLã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    KEYWORDS = {
        'SELECT', 'FROM', 'WHERE', 'INSERT', 'INTO', 'UPDATE', 'DELETE',
        'SET', 'VALUES', 'AND', 'OR', 'IN', 'ALLOW', 'FILTERING',
        'ORDER', 'BY', 'LIMIT', 'USING', 'TTL', 'TIMESTAMP',
        'BEGIN', 'BATCH', 'APPLY', 'UNLOGGED', 'COUNTER'
    }
    
    # æ¯”è¼ƒæ¼”ç®—å­
    OPERATORS = {'=', '>', '<', '>=', '<=', '!='}
    
    def __init__(self, schema_info: Optional[Dict] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            schema_info: ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«å®šç¾©ã€ã‚­ãƒ¼æƒ…å ±ãªã©ï¼‰
                {
                    'table_name': {
                        'partition_keys': ['user_id'],
                        'clustering_keys': ['created_at'],
                        'columns': ['user_id', 'name', 'email', ...]
                    }
                }
        """
        self.schema_info = schema_info or {}
    
    def analyze(self, cql: str) -> CQLAnalysis:
        """
        CQLæ–‡ã‚’åˆ†æ
        
        Args:
            cql: åˆ†æå¯¾è±¡ã®CQLæ–‡
            
        Returns:
            CQLAnalysisã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        if not cql or not cql.strip():
            raise ValueError("CQL cannot be empty")
        
        # æ­£è¦åŒ–ï¼ˆä½™åˆ†ãªç©ºç™½å‰Šé™¤ã€å¤§æ–‡å­—åŒ–ï¼‰
        cql_normalized = self._normalize_cql(cql)
        cql_upper = cql_normalized.upper()
        
        # ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        query_type = self._determine_query_type(cql_upper)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã®æŠ½å‡º
        tables = self._extract_tables(cql_normalized, query_type)
        
        # ALLOW FILTERINGãƒã‚§ãƒƒã‚¯
        has_allow_filtering = 'ALLOW FILTERING' in cql_upper
        
        # WHEREå¥ã®è§£æ
        where_clause = self._parse_where_clause(cql_normalized, tables)
        
        # BATCHå‡¦ç†ãƒã‚§ãƒƒã‚¯
        is_batch = 'BEGIN BATCH' in cql_upper or 'BEGIN UNLOGGED BATCH' in cql_upper
        batch_size = self._count_batch_statements(cql_normalized) if is_batch else 0
        
        # Prepared Statementãƒãƒ¼ã‚«ãƒ¼ãƒã‚§ãƒƒã‚¯
        uses_prepared = '?' in cql
        
        # SELECTå¥ã®è§£æ
        select_columns = []
        if query_type == QueryType.SELECT:
            select_columns = self._extract_select_columns(cql_normalized)
        
        # LIMITå¥
        limit_clause = self._extract_limit(cql_normalized)
        
        # ORDER BYå¥
        order_by_clause = self._extract_order_by(cql_normalized)
        
        # åˆ†æçµæœã®æ§‹ç¯‰
        analysis = CQLAnalysis(
            query_type=query_type,
            tables=tables,
            has_allow_filtering=has_allow_filtering,
            where_clause=where_clause,
            is_batch=is_batch,
            batch_size=batch_size,
            uses_prepared_statement_marker=uses_prepared,
            select_columns=select_columns,
            limit_clause=limit_clause,
            order_by_clause=order_by_clause
        )
        
        # å•é¡Œã®æ¤œå‡º
        analysis.issues = self._detect_issues(analysis, cql)
        
        return analysis
    
    def _normalize_cql(self, cql: str) -> str:
        """
        CQLã‚’æ­£è¦åŒ–
        
        - ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
        - æ”¹è¡Œã‚’ç©ºç™½ã«å¤‰æ›
        - é€£ç¶šã—ãŸç©ºç™½ã‚’1ã¤ã«
        
        Args:
            cql: å…ƒã®CQL
            
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸCQL
        """
        # æ”¹è¡Œã‚’ç©ºç™½ã«
        cql = cql.replace('\n', ' ').replace('\r', ' ')
        
        # é€£ç¶šç©ºç™½ã‚’1ã¤ã«
        cql = re.sub(r'\s+', ' ', cql)
        
        # å‰å¾Œã®ç©ºç™½ã‚’å‰Šé™¤
        cql = cql.strip()
        
        return cql
    
    def _determine_query_type(self, cql_upper: str) -> QueryType:
        """ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—ã®åˆ¤å®š"""
        if cql_upper.startswith('SELECT'):
            return QueryType.SELECT
        elif cql_upper.startswith('INSERT'):
            return QueryType.INSERT
        elif cql_upper.startswith('UPDATE'):
            return QueryType.UPDATE
        elif cql_upper.startswith('DELETE'):
            return QueryType.DELETE
        elif 'BEGIN BATCH' in cql_upper or 'BEGIN UNLOGGED BATCH' in cql_upper:
            return QueryType.BATCH
        elif cql_upper.startswith('CREATE TABLE'):
            return QueryType.CREATE_TABLE
        elif cql_upper.startswith('ALTER TABLE'):
            return QueryType.ALTER_TABLE
        elif cql_upper.startswith('DROP TABLE'):
            return QueryType.DROP_TABLE
        elif cql_upper.startswith('CREATE INDEX'):
            return QueryType.CREATE_INDEX
        elif cql_upper.startswith('TRUNCATE'):
            return QueryType.TRUNCATE
        else:
            return QueryType.UNKNOWN
    
    def _extract_tables(self, cql: str, query_type: QueryType) -> List[str]:
        """
        CQLã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’æŠ½å‡º
        
        Args:
            cql: CQLæ–‡
            query_type: ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ—
            
        Returns:
            ãƒ†ãƒ¼ãƒ–ãƒ«åã®ãƒªã‚¹ãƒˆ
        """
        tables = []
        
        # FROMå¥ï¼ˆSELECT, DELETEï¼‰
        from_match = re.findall(r'FROM\s+(\w+)', cql, re.IGNORECASE)
        tables.extend(from_match)
        
        # INTOå¥ï¼ˆINSERTï¼‰
        into_match = re.findall(r'INTO\s+(\w+)', cql, re.IGNORECASE)
        tables.extend(into_match)
        
        # UPDATEå¥
        if query_type == QueryType.UPDATE:
            update_match = re.findall(r'UPDATE\s+(\w+)', cql, re.IGNORECASE)
            tables.extend(update_match)
        
        # CREATE TABLE
        if query_type == QueryType.CREATE_TABLE:
            create_match = re.findall(r'CREATE\s+TABLE\s+(\w+)', cql, re.IGNORECASE)
            tables.extend(create_match)
        
        # é‡è¤‡å‰Šé™¤
        return list(set(tables))
    
    def _parse_where_clause(
        self, 
        cql: str, 
        tables: List[str]
    ) -> Optional[WhereClause]:
        """
        WHEREå¥ã®è©³ç´°è§£æ
        
        Args:
            cql: CQLæ–‡
            tables: ãƒ†ãƒ¼ãƒ–ãƒ«åãƒªã‚¹ãƒˆ
            
        Returns:
            WhereClauseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€WHEREå¥ãŒãªã„å ´åˆNone
        """
        # WHEREå¥ã®æŠ½å‡º
        where_match = re.search(
            r'WHERE\s+(.+?)(?:ALLOW|ORDER|LIMIT|;|$)', 
            cql, 
            re.IGNORECASE
        )
        
        if not where_match:
            return None
        
        where_text = where_match.group(1).strip()
        
        # æ¡ä»¶ã®è§£æ
        conditions = self._parse_conditions(where_text)
        
        # ãƒ•ã‚£ãƒ«ã‚¿å¯¾è±¡ã‚«ãƒ©ãƒ ã®æŠ½å‡º
        filter_columns = {cond['column'] for cond in conditions}
        
        # Partition Keyåˆ¤å®šï¼ˆã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ãŒã‚ã‚Œã°ï¼‰
        has_partition_key = False
        has_clustering_key = False
        
        if tables and self.schema_info:
            table = tables[0]  # æœ€åˆã®ãƒ†ãƒ¼ãƒ–ãƒ«ã§åˆ¤å®š
            if table in self.schema_info:
                schema = self.schema_info[table]
                partition_keys = set(schema.get('partition_keys', []))
                clustering_keys = set(schema.get('clustering_keys', []))
                
                has_partition_key = bool(filter_columns & partition_keys)
                has_clustering_key = bool(filter_columns & clustering_keys)
        
        # INå¥ã®ä½¿ç”¨
        uses_in = 'IN' in where_text.upper()
        
        # ç¯„å›²ã‚¯ã‚¨ãƒªï¼ˆ>, <, >=, <=ï¼‰ã®ä½¿ç”¨
        uses_range = any(op in where_text for op in ['>', '<', '>=', '<='])
        
        return WhereClause(
            raw_text=where_text,
            conditions=conditions,
            has_partition_key_filter=has_partition_key,
            has_clustering_key_filter=has_clustering_key,
            uses_in_clause=uses_in,
            uses_range_query=uses_range,
            filter_columns=filter_columns
        )
    
    def _parse_conditions(self, where_text: str) -> List[Dict]:
        """
        WHEREå¥ã®æ¡ä»¶ã‚’å€‹åˆ¥ã«è§£æ
        
        Args:
            where_text: WHEREå¥ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ¡ä»¶ã®ãƒªã‚¹ãƒˆ
                [
                    {'column': 'user_id', 'operator': '=', 'value': '?'},
                    {'column': 'status', 'operator': 'IN', 'value': ['active', 'pending']}
                ]
        """
        conditions = []
        
        # AND/ORã§åˆ†å‰²
        parts = re.split(r'\s+AND\s+|\s+OR\s+', where_text, flags=re.IGNORECASE)
        
        for part in parts:
            part = part.strip()
            
            # ç­‰ä¾¡æ¡ä»¶: column = value
            match = re.match(r'(\w+)\s*=\s*(.+)', part)
            if match:
                conditions.append({
                    'column': match.group(1),
                    'operator': '=',
                    'value': match.group(2).strip()
                })
                continue
            
            # INå¥: column IN (...)
            match = re.match(r'(\w+)\s+IN\s*\((.+?)\)', part, re.IGNORECASE)
            if match:
                conditions.append({
                    'column': match.group(1),
                    'operator': 'IN',
                    'value': match.group(2).strip()
                })
                continue
            
            # ç¯„å›²æ¡ä»¶: column > value
            match = re.match(r'(\w+)\s*(>=?|<=?)\s*(.+)', part)
            if match:
                conditions.append({
                    'column': match.group(1),
                    'operator': match.group(2),
                    'value': match.group(3).strip()
                })
                continue
        
        return conditions
    
    def _count_batch_statements(self, cql: str) -> int:
        """BATCHå†…ã®ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        # ã‚»ãƒŸã‚³ãƒ­ãƒ³ã§åˆ†å‰²ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆ
        statements = cql.split(';')
        
        # BEGIN BATCHã¨APPLY BATCHã‚’é™¤å¤–
        count = 0
        for stmt in statements:
            stmt_upper = stmt.strip().upper()
            if stmt_upper and \
               'BEGIN' not in stmt_upper and \
               'APPLY' not in stmt_upper:
                count += 1
        
        return count
    
    def _extract_select_columns(self, cql: str) -> List[str]:
        """SELECTå¯¾è±¡ã‚«ãƒ©ãƒ ã‚’æŠ½å‡º"""
        match = re.search(r'SELECT\s+(.+?)\s+FROM', cql, re.IGNORECASE)
        if not match:
            return []
        
        columns_text = match.group(1).strip()
        
        # SELECT * ã®å ´åˆ
        if columns_text == '*':
            return ['*']
        
        # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ã‚«ãƒ©ãƒ ã‚’æŠ½å‡º
        columns = [col.strip() for col in columns_text.split(',')]
        return columns
    
    def _extract_limit(self, cql: str) -> Optional[int]:
        """LIMITå¥ã‚’æŠ½å‡º"""
        match = re.search(r'LIMIT\s+(\d+)', cql, re.IGNORECASE)
        if match:
            return int(match.group(1))
        return None
    
    def _extract_order_by(self, cql: str) -> Optional[str]:
        """ORDER BYå¥ã‚’æŠ½å‡º"""
        match = re.search(r'ORDER\s+BY\s+(.+?)(?:LIMIT|ALLOW|;|$)', cql, re.IGNORECASE)
        if match:
            return match.group(1).strip()
        return None
    
    def _detect_issues(self, analysis: CQLAnalysis, cql: str) -> List[Dict]:
        """
        å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        
        æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:
        1. ALLOW FILTERING
        2. Partition Keyæœªä½¿ç”¨
        3. å¤§é‡BATCH
        4. SELECT *
        5. INã‚¯ã‚¨ãƒªã®éåº¦ãªä½¿ç”¨
        
        Args:
            analysis: CQLåˆ†æçµæœ
            cql: å…ƒã®CQLæ–‡
            
        Returns:
            å•é¡Œã®ãƒªã‚¹ãƒˆ
        """
        issues = []
        
        # 1. ALLOW FILTERING
        if analysis.has_allow_filtering:
            issues.append({
                'type': 'ALLOW_FILTERING',
                'severity': 'high',
                'message': 'ALLOW FILTERING detected - causes full table scan across all nodes',
                'recommendation': 'Create a Materialized View or redesign the data model to avoid filtering',
                'performance_impact': 'severe'
            })
        
        # 2. Partition Keyæœªä½¿ç”¨ï¼ˆSELECTã®ã¿ï¼‰
        if analysis.query_type == QueryType.SELECT:
            if analysis.where_clause:
                if not analysis.where_clause.has_partition_key_filter:
                    issues.append({
                        'type': 'NO_PARTITION_KEY',
                        'severity': 'critical',
                        'message': 'WHERE clause does not use partition key - requires multi-node scan',
                        'recommendation': 'Add partition key to WHERE clause for single-partition query',
                        'performance_impact': 'severe'
                    })
            else:
                # WHEREå¥ãªã—
                issues.append({
                    'type': 'NO_WHERE_CLAUSE',
                    'severity': 'critical',
                    'message': 'No WHERE clause - scans entire table',
                    'recommendation': 'Add WHERE clause with partition key',
                    'performance_impact': 'critical'
                })
        
        # 3. å¤§é‡BATCH
        if analysis.is_batch and analysis.batch_size > 100:
            issues.append({
                'type': 'LARGE_BATCH',
                'severity': 'medium',
                'message': f'Large batch with {analysis.batch_size} statements - may cause performance issues',
                'recommendation': 'Split batch into chunks of 100 statements or less',
                'performance_impact': 'moderate'
            })
        
        # 4. SELECT *
        if analysis.query_type == QueryType.SELECT:
            if '*' in analysis.select_columns:
                issues.append({
                    'type': 'SELECT_ALL',
                    'severity': 'low',
                    'message': 'SELECT * fetches all columns - may retrieve unnecessary data',
                    'recommendation': 'Specify only required columns explicitly',
                    'performance_impact': 'minor'
                })
        
        # 5. INã‚¯ã‚¨ãƒªã®éåº¦ãªä½¿ç”¨
        if analysis.where_clause and analysis.where_clause.uses_in_clause:
            # INå¥ã®å€¤ã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            in_values_count = cql.count(',')  # è¿‘ä¼¼
            if in_values_count > 10:
                issues.append({
                    'type': 'LARGE_IN_CLAUSE',
                    'severity': 'medium',
                    'message': f'IN clause with many values (approximately {in_values_count}) - may cause performance issues',
                    'recommendation': 'Limit IN clause values to 10 or split into multiple queries',
                    'performance_impact': 'moderate'
                })
        
        return issues
```

---

## 3. ãƒ‡ãƒ¼ã‚¿æ§‹é€ è¨­è¨ˆ

### 3.1 ãƒ¡ãƒ¢ãƒªå†…ãƒ‡ãƒ¼ã‚¿æ§‹é€ 

```python
"""
åˆ†æçµæœã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
"""

from typing import Dict, List, Set
from dataclasses import dataclass, field
from collections import defaultdict

@dataclass
class AnalysisState:
    """
    åˆ†æãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã®çŠ¶æ…‹ã‚’ä¿æŒ
    
    ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªå®Ÿè£…ãŒå¿…è¦
    """
    # ãƒ•ã‚¡ã‚¤ãƒ« â†’ å•é¡Œã®ãƒãƒƒãƒ—
    issues_by_file: Dict[str, List['Issue']] = field(
        default_factory=lambda: defaultdict(list)
    )
    
    # å•é¡Œã‚¿ã‚¤ãƒ— â†’ å•é¡Œã®ãƒãƒƒãƒ—
    issues_by_type: Dict[str, List['Issue']] = field(
        default_factory=lambda: defaultdict(list)
    )
    
    # é‡è¦åº¦ â†’ å•é¡Œã®ãƒãƒƒãƒ—
    issues_by_severity: Dict[str, List['Issue']] = field(
        default_factory=lambda: defaultdict(list)
    )
    
    # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°
    processed_files: int = 0
    
    # ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°
    total_files: int = 0
    
    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ•ã‚¡ã‚¤ãƒ«
    error_files: Set[str] = field(default_factory=set)
    
    # ã‚¹ã‚­ãƒƒãƒ—ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
    skipped_files: Set[str] = field(default_factory=set)
    
    def add_issue(self, issue: 'Issue'):
        """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å•é¡Œã‚’è¿½åŠ """
        self.issues_by_file[issue.file_path].append(issue)
        self.issues_by_type[issue.issue_type].append(issue)
        self.issues_by_severity[issue.severity].append(issue)
    
    def increment_processed(self):
        """å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
        # å®Ÿè£…æ™‚ã¯threading.Lockã‚’ä½¿ç”¨
        self.processed_files += 1
    
    def get_progress(self) -> float:
        """é€²æ—ç‡ã‚’å–å¾—"""
        if self.total_files == 0:
            return 0.0
        return self.processed_files / self.total_files
```

### 3.2 ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹é€ 

```python
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®ãŸã‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
"""

from functools import lru_cache
from typing import Optional
import hashlib

class AnalysisCache:
    """
    åˆ†æçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ + è¨­å®šãƒãƒƒã‚·ãƒ¥
    ã‚­ãƒ£ãƒƒã‚·ãƒ¥å€¤: åˆ†æçµæœ
    """
    
    def __init__(self, max_size: int = 1000):
        """
        Args:
            max_size: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ€å¤§ã‚µã‚¤ã‚º
        """
        self.max_size = max_size
        self._cache: Dict[str, List['Issue']] = {}
        self._access_count: Dict[str, int] = defaultdict(int)
    
    def get(self, file_hash: str, config_hash: str) -> Optional[List['Issue']]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—"""
        key = f"{file_hash}:{config_hash}"
        
        if key in self._cache:
            self._access_count[key] += 1
            return self._cache[key]
        
        return None
    
    def put(self, file_hash: str, config_hash: str, issues: List['Issue']):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        key = f"{file_hash}:{config_hash}"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self._cache) >= self.max_size:
            self._evict_lru()
        
        self._cache[key] = issues
        self._access_count[key] = 1
    
    def _evict_lru(self):
        """LRUæ–¹å¼ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"""
        # ã‚¢ã‚¯ã‚»ã‚¹å›æ•°ãŒæœ€ã‚‚å°‘ãªã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        lru_key = min(self._access_count, key=self._access_count.get)
        del self._cache[lru_key]
        del self._access_count[lru_key]
    
    def invalidate_file(self, file_path: str):
        """ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–"""
        keys_to_delete = [k for k in self._cache.keys() if file_path in k]
        for key in keys_to_delete:
            del self._cache[key]
            del self._access_count[key]
    
    def clear(self):
        """å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        self._cache.clear()
        self._access_count.clear()
```

---

## 4. ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è©³ç´°

### 4.1 ä¸¦åˆ—å‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
"""
ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã®ä¸¦åˆ—å‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable
import multiprocessing

class ParallelAnalyzer:
    """
    ä¸¦åˆ—åˆ†æã®å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³
    
    ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã®æ±ºå®š:
    - CPUæ•°ã®75%ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã™ã‚‹
    - I/Oå¾…ã¡ãŒå¤šã„å‡¦ç†ãªã®ã§CPUæ•°ä»¥ä¸Šã‚‚è¨±å¯
    """
    
    def __init__(self, max_workers: Optional[int] = None):
        """
        Args:
            max_workers: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ±ºå®šï¼‰
        """
        if max_workers is None:
            cpu_count = multiprocessing.cpu_count()
            # CPUæ•°ã®75%ã€æœ€å°2ã€æœ€å¤§16
            max_workers = max(2, min(16, int(cpu_count * 0.75)))
        
        self.max_workers = max_workers
    
    def analyze_files(
        self,
        files: List[Path],
        analyze_func: Callable[[Path], List['Issue']]
    ) -> List['Issue']:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä¸¦åˆ—åˆ†æ
        
        Args:
            files: åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
            analyze_func: å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æé–¢æ•°
            
        Returns:
            å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å•é¡Œãƒªã‚¹ãƒˆ
            
        ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
        1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ã§åˆ†å‰²
        2. å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã«åˆ†æã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
        3. å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’åé›†
        4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°
        """
        all_issues = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ã‚¿ã‚¹ã‚¯ã®æŠ•å…¥
            future_to_file = {
                executor.submit(analyze_func, file): file
                for file in files
            }
            
            # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’åé›†
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                
                try:
                    issues = future.result(timeout=30)  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    all_issues.extend(issues)
                except Exception as e:
                    logger.error(f"Error analyzing {file}: {e}")
        
        return all_issues
```

### 4.2 å•é¡Œã®å„ªå…ˆåº¦ä»˜ã‘ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
"""
æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã«å„ªå…ˆåº¦ã‚’ä»˜ä¸
"""

def calculate_priority_score(issue: 'Issue') -> float:
    """
    å•é¡Œã®å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    
    ã‚¹ã‚³ã‚¢è¨ˆç®—å¼:
        score = severity_weight * confidence * impact_multiplier
    
    Args:
        issue: å•é¡Œ
        
    Returns:
        å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
    """
    # é‡è¦åº¦ã®é‡ã¿
    severity_weights = {
        'critical': 10.0,
        'high': 7.5,
        'medium': 5.0,
        'low': 2.5
    }
    
    base_score = severity_weights.get(issue.severity, 5.0)
    
    # ä¿¡é ¼åº¦ã§èª¿æ•´
    confidence_adjusted = base_score * issue.confidence
    
    # å½±éŸ¿ç¯„å›²ã®ä¿‚æ•°
    impact_multiplier = 1.0
    
    # Cassandraé–¢é€£ã¯ä¿‚æ•°ã‚’ä¸Šã’ã‚‹
    if 'cassandra' in issue.file_path.lower():
        impact_multiplier *= 1.5
    
    # DAOãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ä¿‚æ•°ã‚’ä¸Šã’ã‚‹
    if 'dao' in issue.file_path.lower() or 'repository' in issue.file_path.lower():
        impact_multiplier *= 1.3
    
    final_score = confidence_adjusted * impact_multiplier
    
    # 0-100ã«æ­£è¦åŒ–
    return min(100, final_score * 10)

def sort_issues_by_priority(issues: List['Issue']) -> List['Issue']:
    """å•é¡Œã‚’å„ªå…ˆåº¦é †ã«ã‚½ãƒ¼ãƒˆ"""
    return sorted(
        issues,
        key=lambda issue: (
            calculate_priority_score(issue),
            issue.severity,
            issue.file_path
        ),
        reverse=True
    )
```

### 4.3 é‡è¤‡æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
"""
åŒä¸€å•é¡Œã®é‡è¤‡ã‚’æ¤œå‡º
"""

def deduplicate_issues(issues: List['Issue']) -> List['Issue']:
    """
    é‡è¤‡ã™ã‚‹å•é¡Œã‚’é™¤å»
    
    é‡è¤‡åˆ¤å®šåŸºæº–:
    - åŒã˜ãƒ•ã‚¡ã‚¤ãƒ«
    - åŒã˜è¡Œç•ªå·
    - åŒã˜å•é¡Œã‚¿ã‚¤ãƒ—
    - CQLæ–‡å­—åˆ—ãŒé¡ä¼¼ï¼ˆç·¨é›†è·é›¢ã§åˆ¤å®šï¼‰
    
    Args:
        issues: å•é¡Œãƒªã‚¹ãƒˆ
        
    Returns:
        é‡è¤‡é™¤å»å¾Œã®å•é¡Œãƒªã‚¹ãƒˆ
    """
    unique_issues = []
    seen_signatures = set()
    
    for issue in issues:
        # å•é¡Œã®ã‚·ã‚°ãƒãƒãƒ£ã‚’ç”Ÿæˆ
        signature = _generate_issue_signature(issue)
        
        if signature not in seen_signatures:
            unique_issues.append(issue)
            seen_signatures.add(signature)
        else:
            logger.debug(f"Duplicate issue filtered: {issue}")
    
    return unique_issues

def _generate_issue_signature(issue: 'Issue') -> str:
    """
    å•é¡Œã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚·ã‚°ãƒãƒãƒ£ã‚’ç”Ÿæˆ
    
    Returns:
        ãƒãƒƒã‚·ãƒ¥æ–‡å­—åˆ—
    """
    import hashlib
    
    components = [
        issue.file_path,
        str(issue.line_number),
        issue.issue_type,
        issue.cql_text[:100]  # CQLæœ€åˆã®100æ–‡å­—
    ]
    
    signature_str = '|'.join(components)
    return hashlib.md5(signature_str.encode()).hexdigest()
```

---

## 5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°è¨­è¨ˆ

### 5.1 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥

```python
"""
åŒ…æ‹¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""

from typing import Optional, Any
from dataclasses import dataclass
import traceback

@dataclass
class AnalysisError:
    """åˆ†æã‚¨ãƒ©ãƒ¼ã®è©³ç´°"""
    file_path: str
    error_type: str
    error_message: str
    stack_trace: str
    recoverable: bool
    
class ErrorHandler:
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£"""
    
    def __init__(self):
        self.errors: List[AnalysisError] = []
    
    def handle_parse_error(
        self,
        file_path: Path,
        exception: Exception
    ) -> Optional[List['Issue']]:
        """
        ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        
        æˆ¦ç•¥:
        1. javalang.JavaSyntaxError â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—
        2. UnicodeDecodeError â†’ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å†è©¦è¡Œ
        3. ãã®ä»– â†’ ãƒ­ã‚°ã—ã¦ç¶™ç¶š
        
        Returns:
            å›å¾©å¯èƒ½ãªå ´åˆã¯å†è©¦è¡Œçµæœã€ä¸å¯èƒ½ãªå ´åˆNone
        """
        if isinstance(exception, javalang.parser.JavaSyntaxError):
            # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ - ã‚¹ã‚­ãƒƒãƒ—
            logger.warning(f"Syntax error in {file_path}, skipping: {exception}")
            self._record_error(file_path, exception, recoverable=False)
            return None
        
        elif isinstance(exception, UnicodeDecodeError):
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ - åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å†è©¦è¡Œ
            logger.info(f"Retrying {file_path} with different encoding")
            try:
                return self._retry_with_latin1(file_path)
            except Exception as e:
                self._record_error(file_path, e, recoverable=False)
                return None
        
        else:
            # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼
            logger.error(f"Unexpected error in {file_path}: {exception}")
            self._record_error(file_path, exception, recoverable=False)
            return None
    
    def handle_detection_error(
        self,
        issue_type: str,
        call: 'CassandraCall',
        exception: Exception
    ) -> None:
        """
        æ¤œå‡ºå™¨ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        
        æ¤œå‡ºå™¨ã®ã‚¨ãƒ©ãƒ¼ã¯å…¨ä½“ã®å‡¦ç†ã‚’æ­¢ã‚ãªã„
        """
        logger.error(
            f"Detector '{issue_type}' failed on {call.file_path}:{call.line_number}: "
            f"{exception}"
        )
        self._record_error(call.file_path, exception, recoverable=True)
    
    def _record_error(
        self,
        file_path: Path,
        exception: Exception,
        recoverable: bool
    ):
        """ã‚¨ãƒ©ãƒ¼ã‚’è¨˜éŒ²"""
        error = AnalysisError(
            file_path=str(file_path),
            error_type=type(exception).__name__,
            error_message=str(exception),
            stack_trace=traceback.format_exc(),
            recoverable=recoverable
        )
        self.errors.append(error)
    
    def _retry_with_latin1(self, file_path: Path) -> List['Issue']:
        """Latin-1ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§å†è©¦è¡Œ"""
        # å®Ÿè£…çœç•¥
        pass
    
    def get_error_summary(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        return {
            'total_errors': len(self.errors),
            'recoverable': sum(1 for e in self.errors if e.recoverable),
            'unrecoverable': sum(1 for e in self.errors if not e.recoverable),
            'by_type': self._group_errors_by_type()
        }
    
    def _group_errors_by_type(self) -> Dict[str, int]:
        """ã‚¨ãƒ©ãƒ¼ã‚’ã‚¿ã‚¤ãƒ—åˆ¥ã«é›†è¨ˆ"""
        from collections import Counter
        return dict(Counter(e.error_type for e in self.errors))
```

### 5.2 ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç®¡ç†

```python
"""
å‡¦ç†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç®¡ç†
"""

import signal
from contextlib import contextmanager

@contextmanager
def timeout(seconds: int):
    """
    å‡¦ç†ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š
    
    ä½¿ç”¨ä¾‹:
        with timeout(30):
            result = long_running_function()
    """
    def timeout_handler(signum, frame):
        raise TimeoutError(f"Operation timed out after {seconds} seconds")
    
    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(seconds)
    
    try:
        yield
    finally:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è§£é™¤
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

# ä½¿ç”¨ä¾‹
def analyze_with_timeout(file_path: Path, timeout_seconds: int = 30) -> List['Issue']:
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§åˆ†æ"""
    try:
        with timeout(timeout_seconds):
            return analyze_file(file_path)
    except TimeoutError as e:
        logger.warning(f"Analysis of {file_path} timed out after {timeout_seconds}s")
        return []
```

---

## 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­è¨ˆ

### 6.1 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• |
|-----|--------|---------|
| å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«è§£æ | < 100ms | å¹³å‡å®Ÿè¡Œæ™‚é–“ |
| 10ãƒ•ã‚¡ã‚¤ãƒ«ä¸¦åˆ—è§£æ | < 1ç§’ | ç·å®Ÿè¡Œæ™‚é–“ |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | < 500MB | 20ãƒ•ã‚¡ã‚¤ãƒ«è§£ææ™‚ |
| ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ | > 80% | å†è§£ææ™‚ |

### 6.2 æœ€é©åŒ–æ‰‹æ³•

```python
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
"""

# 1. é…å»¶è©•ä¾¡
class LazyParser:
    """å¿…è¦ã«ãªã‚‹ã¾ã§ãƒ‘ãƒ¼ã‚¹ã‚’é…å»¶"""
    
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self._content: Optional[str] = None
        self._ast: Optional[Any] = None
    
    @property
    def content(self) -> str:
        """é…å»¶èª­ã¿è¾¼ã¿"""
        if self._content is None:
            with open(self.file_path) as f:
                self._content = f.read()
        return self._content
    
    @property
    def ast(self) -> Any:
        """é…å»¶ãƒ‘ãƒ¼ã‚¹"""
        if self._ast is None:
            self._ast = javalang.parse.parse(self.content)
        return self._ast

# 2. ãƒãƒƒãƒå‡¦ç†
def analyze_in_batches(
    files: List[Path],
    batch_size: int = 10
) -> List['Issue']:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒãƒå˜ä½ã§å‡¦ç†ã—ã¦ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–"""
    all_issues = []
    
    for i in range(0, len(files), batch_size):
        batch = files[i:i+batch_size]
        batch_issues = analyze_batch(batch)
        all_issues.extend(batch_issues)
        
        # ãƒãƒƒãƒé–“ã§ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢
        gc.collect()
    
    return all_issues

# 3. ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«è§£æ
class IncrementalAnalyzer:
    """
    å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å†è§£æ
    
    å‰å›ã®åˆ†æçµæœã¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’ä¿å­˜ã—ã€
    å¤‰æ›´ãŒãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
    """
    
    def __init__(self, state_file: Path):
        self.state_file = state_file
        self.previous_state = self._load_state()
    
    def analyze_incremental(self, files: List[Path]) -> List['Issue']:
        """å¢—åˆ†è§£æ"""
        files_to_analyze = []
        cached_issues = []
        
        for file in files:
            file_hash = compute_file_hash(file)
            
            if file_hash in self.previous_state:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
                cached_issues.extend(self.previous_state[file_hash]['issues'])
            else:
                # æ–°è¦ã¾ãŸã¯å¤‰æ›´ã‚ã‚Š
                files_to_analyze.append(file)
        
        # å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è§£æ
        new_issues = analyze_files(files_to_analyze)
        
        return cached_issues + new_issues
    
    def _load_state(self) -> Dict:
        """å‰å›ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã¿"""
        if self.state_file.exists():
            with open(self.state_file) as f:
                return json.load(f)
        return {}
```

---

## 7. ãƒ†ã‚¹ãƒˆè¨­è¨ˆ

### 7.1 ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   E2E    â”‚  10% - å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å‹•ä½œç¢ºèª
         â”‚  Tests   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Integration  â”‚  30% - ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®é€£æº
       â”‚    Tests     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   Unit Tests     â”‚  60% - å€‹åˆ¥ã‚¯ãƒ©ã‚¹ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰
     â”‚                  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹è¨­è¨ˆ

```python
"""
åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
"""

import pytest
from pathlib import Path

# === ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ ===

class TestJavaParser:
    """JavaParserã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def parser(self):
        return JavaCassandraParser()
    
    @pytest.fixture
    def sample_code(self):
        return """
        public class UserDAO {
            private Session session;
            
            public User findById(String id) {
                String cql = "SELECT * FROM users WHERE user_id = ?";
                PreparedStatement ps = session.prepare(cql);
                BoundStatement bound = ps.bind(id);
                ResultSet rs = session.execute(bound);
                return mapToUser(rs.one());
            }
        }
        """
    
    def test_parse_simple_select(self, parser, sample_code, tmp_path):
        """ã‚·ãƒ³ãƒ—ãƒ«ãªSELECTã®ãƒ‘ãƒ¼ã‚¹"""
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        test_file = tmp_path / "UserDAO.java"
        test_file.write_text(sample_code)
        
        # ãƒ‘ãƒ¼ã‚¹å®Ÿè¡Œ
        calls = parser.parse_file(test_file)
        
        # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
        assert len(calls) == 1
        assert calls[0].call_type == CallType.PREPARE
        assert "SELECT * FROM users" in calls[0].cql_text
        assert calls[0].is_prepared == True
    
    def test_parse_allow_filtering(self, parser, tmp_path):
        """ALLOW FILTERINGã‚’å«ã‚€ã‚³ãƒ¼ãƒ‰ã®ãƒ‘ãƒ¼ã‚¹"""
        code = """
        public List<User> findByEmail(String email) {
            String cql = "SELECT * FROM users WHERE email = ? ALLOW FILTERING";
            return session.execute(cql, email).all();
        }
        """
        
        test_file = tmp_path / "BadDAO.java"
        test_file.write_text(code)
        
        calls = parser.parse_file(test_file)
        
        assert len(calls) == 1
        assert "ALLOW FILTERING" in calls[0].cql_text
    
    def test_parse_batch(self, parser, tmp_path):
        """BATCHå‡¦ç†ã®ãƒ‘ãƒ¼ã‚¹"""
        code = """
        public void updateMultiple(List<User> users) {
            BatchStatement batch = new BatchStatement();
            for (User user : users) {
                batch.add(session.prepare("UPDATE users SET name = ? WHERE id = ?")
                    .bind(user.getName(), user.getId()));
            }
            session.execute(batch);
        }
        """
        
        test_file = tmp_path / "BatchDAO.java"
        test_file.write_text(code)
        
        calls = parser.parse_file(test_file)
        
        assert len(calls) > 0
        # BATCHã®æ¤œå‡ºã‚’ç¢ºèª

class TestCQLParser:
    """CQLParserã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def parser(self):
        # ã‚¹ã‚­ãƒ¼ãƒæƒ…å ±ä»˜ãã§åˆæœŸåŒ–
        schema_info = {
            'users': {
                'partition_keys': ['user_id'],
                'clustering_keys': ['created_at'],
                'columns': ['user_id', 'name', 'email', 'created_at']
            }
        }
        return CQLParser(schema_info=schema_info)
    
    def test_detect_allow_filtering(self, parser):
        """ALLOW FILTERINGæ¤œå‡º"""
        cql = "SELECT * FROM users WHERE email = 'test@example.com' ALLOW FILTERING"
        analysis = parser.analyze(cql)
        
        assert analysis.has_allow_filtering == True
        assert len(analysis.issues) > 0
        assert analysis.issues[0]['type'] == 'ALLOW_FILTERING'
    
    def test_detect_no_partition_key(self, parser):
        """Partition Keyæœªä½¿ç”¨æ¤œå‡º"""
        cql = "SELECT * FROM users WHERE email = ?"
        analysis = parser.analyze(cql)
        
        assert analysis.where_clause is not None
        assert analysis.where_clause.has_partition_key_filter == False
        assert any(i['type'] == 'NO_PARTITION_KEY' for i in analysis.issues)
    
    def test_batch_size_detection(self, parser):
        """å¤§é‡BATCHã®æ¤œå‡º"""
        # 150å€‹ã®INSERTã‚’å«ã‚€BATCH
        statements = [
            f"INSERT INTO users (user_id, name) VALUES ('{i}', 'User{i}');"
            for i in range(150)
        ]
        cql = "BEGIN BATCH\n" + "\n".join(statements) + "\nAPPLY BATCH;"
        
        analysis = parser.analyze(cql)
        
        assert analysis.is_batch == True
        assert analysis.batch_size == 150
        assert any(i['type'] == 'LARGE_BATCH' for i in analysis.issues)

# === çµ±åˆãƒ†ã‚¹ãƒˆ ===

class TestAnalysisPipeline:
    """åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def pipeline(self):
        config = {
            'detection': {
                'allow_filtering': {'enabled': True, 'severity': 'high'},
                'partition_key': {'enabled': True, 'severity': 'critical'},
                'batch_size': {'enabled': True, 'threshold': 100, 'severity': 'medium'},
                'prepared_statement': {'enabled': True, 'min_executions': 5, 'severity': 'low'}
            }
        }
        return AnalysisPipeline(config)
    
    def test_analyze_file_with_issues(self, pipeline, tmp_path):
        """å•é¡Œã‚’å«ã‚€ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ"""
        code = """
        public class ProblematicDAO {
            private Session session;
            
            public List<User> findAll() {
                // å•é¡Œ1: WHEREå¥ãªã—
                String cql = "SELECT * FROM users";
                return session.execute(cql).all();
            }
            
            public User findByEmail(String email) {
                // å•é¡Œ2: ALLOW FILTERING
                String cql = "SELECT * FROM users WHERE email = ? ALLOW FILTERING";
                return session.execute(cql, email).one();
            }
        }
        """
        
        test_file = tmp_path / "ProblematicDAO.java"
        test_file.write_text(code)
        
        issues = pipeline.analyze_file(test_file)
        
        # 2ã¤ä»¥ä¸Šã®å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã‚‹ã¹ã
        assert len(issues) >= 2
        
        # ALLOW FILTERINGãŒæ¤œå‡ºã•ã‚Œã¦ã„ã‚‹ã¹ã
        assert any(i.issue_type == 'ALLOW_FILTERING' for i in issues)

# === E2Eãƒ†ã‚¹ãƒˆ ===

class TestEndToEnd:
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ"""
    
    def test_full_analysis_workflow(self, tmp_path):
        """å®Œå…¨ãªåˆ†æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
        # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’ä½œæˆ
        project_dir = tmp_path / "test_project"
        dao_dir = project_dir / "src" / "main" / "java" / "com" / "example" / "dao"
        dao_dir.mkdir(parents=True)
        
        # è¤‡æ•°ã®DAOãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        (dao_dir / "UserDAO.java").write_text("""
        public class UserDAO {
            public User findById(String id) {
                String cql = "SELECT * FROM users WHERE user_id = ?";
                PreparedStatement ps = session.prepare(cql);
                return session.execute(ps.bind(id)).one();
            }
        }
        """)
        
        (dao_dir / "OrderDAO.java").write_text("""
        public class OrderDAO {
            public List<Order> findByStatus(String status) {
                // ALLOW FILTERINGå•é¡Œ
                String cql = "SELECT * FROM orders WHERE status = ? ALLOW FILTERING";
                return session.execute(cql, status).all();
            }
        }
        """)
        
        # åˆ†æå®Ÿè¡Œ
        orchestrator = AnalysisOrchestrator(config={})
        result = orchestrator.analyze(project_dir)
        
        # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
        assert len(result.analyzed_files) == 2
        assert result.total_issues > 0
        assert 'high' in result.issues_by_severity
```

### 7.3 ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿

```python
"""
ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ãƒ‡ãƒ¼ã‚¿
"""

# tests/fixtures/sample_good.java
GOOD_DAO_CODE = """
public class GoodUserDAO {
    private Session session;
    
    public User findById(String userId) {
        // è‰¯ã„ä¾‹: Prepared Statement + Partition Keyä½¿ç”¨
        String cql = "SELECT user_id, name, email FROM users WHERE user_id = ?";
        PreparedStatement ps = session.prepare(cql);
        BoundStatement bound = ps.bind(userId);
        bound.setConsistencyLevel(ConsistencyLevel.LOCAL_QUORUM);
        ResultSet rs = session.execute(bound);
        return mapToUser(rs.one());
    }
    
    public List<Order> findUserOrders(String userId, Date from, Date to) {
        // è‰¯ã„ä¾‹: Partition Key + Clustering Keyç¯„å›²ã‚¯ã‚¨ãƒª
        String cql = "SELECT * FROM orders WHERE user_id = ? AND created_at >= ? AND created_at < ?";
        PreparedStatement ps = session.prepare(cql);
        return session.execute(ps.bind(userId, from, to)).all();
    }
}
"""

# tests/fixtures/sample_bad1.java - ALLOW FILTERING
BAD_DAO_ALLOW_FILTERING = """
public class BadUserDAO {
    public List<User> findByEmail(String email) {
        // æ‚ªã„ä¾‹: ALLOW FILTERINGä½¿ç”¨
        String cql = "SELECT * FROM users WHERE email = ? ALLOW FILTERING";
        return session.execute(cql, email).all();
    }
}
"""

# tests/fixtures/sample_bad2.java - Partition Keyæœªä½¿ç”¨
BAD_DAO_NO_PARTITION_KEY = """
public class BadOrderDAO {
    public List<Order> findByStatus(String status) {
        // æ‚ªã„ä¾‹: Partition Keyæœªä½¿ç”¨
        String cql = "SELECT * FROM orders WHERE status = ?";
        return session.execute(cql, status).all();
    }
}
"""

# tests/fixtures/sample_bad3.java - å¤§é‡BATCH
BAD_DAO_LARGE_BATCH = """
public class BadBatchDAO {
    public void insertMany(List<User> users) {
        // æ‚ªã„ä¾‹: å¤§é‡ã®BATCHå‡¦ç†
        BatchStatement batch = new BatchStatement();
        for (User user : users) {  // usersãŒ200ä»¶ã¨ä»®å®š
            batch.add(session.prepare("INSERT INTO users (user_id, name) VALUES (?, ?)")
                .bind(user.getId(), user.getName()));
        }
        session.execute(batch);
    }
}
"""
```

---

## 8. ãƒ­ã‚°ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°è¨­è¨ˆ

### 8.1 ãƒ­ã‚°è¨­è¨ˆ

```python
"""
æ§‹é€ åŒ–ãƒ­ã‚°ã®å®Ÿè£…
"""

import logging
import json
from typing import Dict, Any
from datetime import datetime

class StructuredLogger:
    """
    æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›
    
    ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: JSON Lines
    {
        "timestamp": "2025-10-26T10:30:00Z",
        "level": "INFO",
        "component": "JavaParser",
        "event": "file_parsed",
        "file_path": "/path/to/UserDAO.java",
        "duration_ms": 45,
        "calls_found": 3
    }
    """
    
    def __init__(self, component: str):
        self.component = component
        self.logger = logging.getLogger(component)
    
    def log(self, level: str, event: str, **kwargs):
        """æ§‹é€ åŒ–ãƒ­ã‚°ã‚’å‡ºåŠ›"""
        log_entry = {
            'timestamp': datetime.utcnow().isoformat() + 'Z',
            'level': level,
            'component': self.component,
            'event': event,
            **kwargs
        }
        
        log_line = json.dumps(log_entry)
        
        if level == 'ERROR':
            self.logger.error(log_line)
        elif level == 'WARNING':
            self.logger.warning(log_line)
        elif level == 'INFO':
            self.logger.info(log_line)
        else:
            self.logger.debug(log_line)
    
    def log_performance(self, operation: str, duration_ms: float, **kwargs):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°"""
        self.log(
            'INFO',
            'performance',
            operation=operation,
            duration_ms=duration_ms,
            **kwargs
        )
    
    def log_error(self, error: Exception, **kwargs):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°"""
        import traceback
        self.log(
            'ERROR',
            'error_occurred',
            error_type=type(error).__name__,
            error_message=str(error),
            stack_trace=traceback.format_exc(),
            **kwargs
        )

# ä½¿ç”¨ä¾‹
logger = StructuredLogger('JavaParser')

# ãƒ•ã‚¡ã‚¤ãƒ«è§£æã®é–‹å§‹
logger.log('INFO', 'parse_start', file_path='/path/to/UserDAO.java')

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
start = time.time()
# ... å‡¦ç† ...
duration = (time.time() - start) * 1000
logger.log_performance('file_parse', duration, file_path='/path/to/UserDAO.java')

# ã‚¨ãƒ©ãƒ¼
try:
    # ... å‡¦ç† ...
    pass
except Exception as e:
    logger.log_error(e, file_path='/path/to/UserDAO.java')
```

### 8.2 ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

```python
"""
ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†ã¨å‡ºåŠ›
"""

from dataclasses import dataclass, field
from typing import Dict
import time

@dataclass
class AnalysisMetrics:
    """åˆ†æã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«æ•°
    total_files: int = 0
    processed_files: int = 0
    error_files: int = 0
    skipped_files: int = 0
    
    # å•é¡Œæ•°
    total_issues: int = 0
    critical_issues: int = 0
    high_issues: int = 0
    medium_issues: int = 0
    low_issues: int = 0
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
    total_duration_seconds: float = 0.0
    average_file_duration_ms: float = 0.0
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    cache_hits: int = 0
    cache_misses: int = 0
    
    def to_dict(self) -> Dict:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            'files': {
                'total': self.total_files,
                'processed': self.processed_files,
                'errors': self.error_files,
                'skipped': self.skipped_files
            },
            'issues': {
                'total': self.total_issues,
                'by_severity': {
                    'critical': self.critical_issues,
                    'high': self.high_issues,
                    'medium': self.medium_issues,
                    'low': self.low_issues
                }
            },
            'performance': {
                'total_duration_seconds': self.total_duration_seconds,
                'average_file_duration_ms': self.average_file_duration_ms
            },
            'cache': {
                'hits': self.cache_hits,
                'misses': self.cache_misses,
                'hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses)
                    if (self.cache_hits + self.cache_misses) > 0 else 0.0
            }
        }
    
    def print_summary(self):
        """ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›"""
        print("\n" + "="*60)
        print("Analysis Summary")
        print("="*60)
        print(f"Files Analyzed: {self.processed_files}/{self.total_files}")
        print(f"Total Issues: {self.total_issues}")
        print(f"  Critical: {self.critical_issues}")
        print(f"  High: {self.high_issues}")
        print(f"  Medium: {self.medium_issues}")
        print(f"  Low: {self.low_issues}")
        print(f"\nPerformance:")
        print(f"  Total Time: {self.total_duration_seconds:.2f}s")
        print(f"  Avg per File: {self.average_file_duration_ms:.1f}ms")
        print(f"\nCache:")
        hit_rate = self.cache_hits / (self.cache_hits + self.cache_misses) * 100 \
            if (self.cache_hits + self.cache_misses) > 0 else 0.0
        print(f"  Hit Rate: {hit_rate:.1f}%")
        print("="*60)
```

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ã“ã®è©³ç´°è¨­è¨ˆæ›¸ã«åŸºã¥ã„ã¦ã€TODOç®¡ç†ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§å…·ä½“çš„ãªå®Ÿè£…ã‚¿ã‚¹ã‚¯ã‚’å®šç¾©ã—ã¾ã™ã€‚

**å®Ÿè£…ã®é–‹å§‹æº–å‚™**:
1. é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
2. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä½œæˆ
3. ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
4. ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã®æº–å‚™

å„ã‚¯ãƒ©ã‚¹ã®å®Ÿè£…ã¯ã€Claude Code CLIã§æ®µéšçš„ã«é€²ã‚ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚
