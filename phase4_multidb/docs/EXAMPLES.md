# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰é›†

**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: v1.0.0
**æœ€çµ‚æ›´æ–°**: 2025å¹´01æœˆ27æ—¥

## ç›®æ¬¡

1. [åŸºæœ¬çš„ãªä½¿ã„æ–¹](#åŸºæœ¬çš„ãªä½¿ã„æ–¹)
2. [ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨ã®ä½œæˆ](#ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨ã®ä½œæˆ)
3. [LLMçµ±åˆã®å®Ÿè·µä¾‹](#llmçµ±åˆã®å®Ÿè·µä¾‹)
4. [ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ](#ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ)
5. [CI/CDçµ±åˆ](#cicdçµ±åˆ)
6. [é«˜åº¦ãªä½¿ç”¨ä¾‹](#é«˜åº¦ãªä½¿ç”¨ä¾‹)

---

## åŸºæœ¬çš„ãªä½¿ã„æ–¹

### ä¾‹1: ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†æ

```python
"""
æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†æä¾‹
"""
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import (
    WildcardDetector,
    NPlusOneDetector,
    LargeSizeDetector
)
from multidb_analyzer.core.base_detector import AnalysisContext

# Javaãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
with open('SearchService.java', 'r', encoding='utf-8') as f:
    code = f.read()

# ãƒ‘ãƒ¼ã‚µãƒ¼ã§ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
parser = JavaElasticsearchParser()
queries = parser.parse_file('SearchService.java', code)

print(f"Found {len(queries)} queries")

# æ¤œå‡ºå™¨ã‚’å®Ÿè¡Œ
context = AnalysisContext(
    file_path='SearchService.java',
    code_content=code
)

detectors = [
    WildcardDetector(),
    NPlusOneDetector(),
    LargeSizeDetector()
]

all_issues = []
for detector in detectors:
    issues = detector.detect(queries, context)
    all_issues.extend(issues)
    print(f"{detector.__class__.__name__}: {len(issues)} issues")

# çµæœã‚’è¡¨ç¤º
for issue in all_issues:
    print(f"\n{issue.severity.value}: {issue.title}")
    print(f"  File: {issue.file_path}:{issue.line_number}")
    print(f"  Description: {issue.description}")
    print(f"  Suggestion: {issue.suggestion}")
```

### ä¾‹2: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã‚’åˆ†æ

```python
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã‚’åˆ†æã™ã‚‹ä¾‹
"""
from pathlib import Path
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import get_all_detectors
from multidb_analyzer.core.base_detector import AnalysisContext

def analyze_project(project_dir: str):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®å…¨Javaãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
    parser = JavaElasticsearchParser()
    detectors = get_all_detectors()

    all_issues = []

    # å…¨Javaãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    project_path = Path(project_dir)
    java_files = list(project_path.rglob('*.java'))

    print(f"Analyzing {len(java_files)} Java files...")

    for java_file in java_files:
        try:
            with open(java_file, 'r', encoding='utf-8') as f:
                code = f.read()

            # ã‚¯ã‚¨ãƒªã‚’æŠ½å‡º
            queries = parser.parse_file(str(java_file), code)

            if not queries:
                continue

            # æ¤œå‡ºå™¨ã‚’å®Ÿè¡Œ
            context = AnalysisContext(
                file_path=str(java_file),
                code_content=code
            )

            for detector in detectors:
                issues = detector.detect(queries, context)
                all_issues.extend(issues)

        except Exception as e:
            print(f"Error analyzing {java_file}: {e}")
            continue

    return all_issues

# å®Ÿè¡Œ
if __name__ == '__main__':
    issues = analyze_project('/path/to/project/src')

    # é‡å¤§åº¦åˆ¥ã«é›†è¨ˆ
    from collections import Counter
    severity_counts = Counter(issue.severity.value for issue in issues)

    print("\n=== Analysis Summary ===")
    for severity, count in severity_counts.most_common():
        print(f"{severity}: {count} issues")
```

### ä¾‹3: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã£ãŸåˆ†æ

```python
"""
YAMLã‚³ãƒ³ãƒ•ã‚£ã‚°ã‚’ä½¿ã£ãŸåˆ†æä¾‹
"""
import yaml
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import (
    WildcardDetector,
    LargeSizeDetector
)
from multidb_analyzer.core.base_detector import DetectorConfig, AnalysisContext

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# æ¤œå‡ºå™¨ã‚’è¨­å®šä»˜ãã§åˆæœŸåŒ–
wildcard_config = DetectorConfig(
    enabled=config['detectors']['wildcard']['enabled'],
    severity_override=config['detectors']['wildcard'].get('severity')
)

large_size_config = DetectorConfig(
    custom_params={
        'size_threshold': config['detectors']['large_size']['size_threshold']
    }
)

detectors = [
    WildcardDetector(config=wildcard_config),
    LargeSizeDetector(config=large_size_config)
]

# åˆ†æå®Ÿè¡Œ
parser = JavaElasticsearchParser()
# ... ä»¥ä¸‹ã¯ä¾‹1ã¨åŒæ§˜
```

**config.yaml**:
```yaml
detectors:
  wildcard:
    enabled: true
    severity: CRITICAL

  large_size:
    enabled: true
    size_threshold: 500
```

---

## ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨ã®ä½œæˆ

### ä¾‹4: ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨ã®å®Ÿè£…

```python
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ãƒ«ãƒ¼ãƒ«ã‚’æŒã¤ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨
"""
from typing import List
from multidb_analyzer.core.base_detector import (
    BaseDetector,
    Issue,
    Severity,
    IssueCategory,
    AnalysisContext
)
from multidb_analyzer.elasticsearch.models import ElasticsearchQuery

class CustomAggregationDetector(BaseDetector):
    """
    è¤‡é›‘ãªã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ¤œå‡ºã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨
    """

    def __init__(self, config=None):
        super().__init__(config)
        self.name = "CustomAggregationDetector"

    def detect(
        self,
        queries: List[ElasticsearchQuery],
        context: AnalysisContext
    ) -> List[Issue]:
        """ã‚¢ã‚°ãƒªã‚²ãƒ¼ã‚·ãƒ§ãƒ³é–¢é€£ã®å•é¡Œã‚’æ¤œå‡º"""
        issues = []

        for query in queries:
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯: 5éšå±¤ä»¥ä¸Šã®ãƒã‚¹ãƒˆã‚’æ¤œå‡º
            if self._has_deep_nested_aggregations(query):
                issues.append(Issue(
                    detector_name=self.name,
                    severity=Severity.HIGH,
                    category=IssueCategory.PERFORMANCE,
                    title="Deep Nested Aggregations",
                    description=(
                        "Query contains deeply nested aggregations (>5 levels) "
                        "which may cause performance issues and high memory usage."
                    ),
                    file_path=context.file_path,
                    line_number=query.line_number,
                    query_text=query.raw_code,
                    suggestion=(
                        "Consider:\n"
                        "1. Flattening the aggregation structure\n"
                        "2. Using composite aggregations\n"
                        "3. Splitting into multiple queries"
                    )
                ))

            # ã‚«ã‚¹ã‚¿ãƒ ãƒ­ã‚¸ãƒƒã‚¯: ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£é›†ç´„ã®èª¤ç”¨ã‚’æ¤œå‡º
            if self._has_high_cardinality_aggregation(query):
                issues.append(Issue(
                    detector_name=self.name,
                    severity=Severity.MEDIUM,
                    category=IssueCategory.PERFORMANCE,
                    title="High Cardinality Aggregation",
                    description=(
                        "Aggregation on high cardinality field without proper limits"
                    ),
                    file_path=context.file_path,
                    line_number=query.line_number,
                    query_text=query.raw_code,
                    suggestion="Use 'size' parameter to limit results"
                ))

        return issues

    def _has_deep_nested_aggregations(self, query: ElasticsearchQuery) -> bool:
        """ãƒã‚¹ãƒˆã®æ·±ã•ã‚’ãƒã‚§ãƒƒã‚¯"""
        code = query.raw_code.lower()
        # ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…: "aggregation("ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        return code.count('aggregation(') > 5

    def _has_high_cardinality_aggregation(self, query: ElasticsearchQuery) -> bool:
        """é«˜ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¸ã®é›†ç´„ã‚’ãƒã‚§ãƒƒã‚¯"""
        code = query.raw_code.lower()
        high_cardinality_fields = ['user_id', 'session_id', 'ip_address']

        for field in high_cardinality_fields:
            if f'terms("{field}"' in code or f"terms('{field}'" in code:
                # sizeãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if '.size(' not in code:
                    return True
        return False

# ä½¿ç”¨ä¾‹
if __name__ == '__main__':
    from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser

    parser = JavaElasticsearchParser()
    custom_detector = CustomAggregationDetector()

    with open('AnalyticsService.java', 'r') as f:
        code = f.read()

    queries = parser.parse_file('AnalyticsService.java', code)
    context = AnalysisContext(
        file_path='AnalyticsService.java',
        code_content=code
    )

    issues = custom_detector.detect(queries, context)
    print(f"Found {len(issues)} custom issues")
```

---

## LLMçµ±åˆã®å®Ÿè·µä¾‹

### ä¾‹5: åŸºæœ¬çš„ãªLLMåˆ†æ

```python
"""
LLMã‚’ä½¿ã£ãŸå•é¡Œåˆ†æã®åŸºæœ¬ä¾‹
"""
from multidb_analyzer.llm import LLMOptimizer, ClaudeModel
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import WildcardDetector
from multidb_analyzer.core.base_detector import AnalysisContext

# LLM Optimizerã‚’åˆæœŸåŒ–
optimizer = LLMOptimizer(
    api_key="sk-ant-xxxxx",  # ã¾ãŸã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰
    model=ClaudeModel.SONNET,
    temperature=0.3  # æŠ€è¡“çš„ãªåˆ†æã«ã¯ä½æ¸©åº¦
)

# ã‚³ãƒ¼ãƒ‰ã‚’åˆ†æ
with open('SearchService.java', 'r', encoding='utf-8') as f:
    code = f.read()

parser = JavaElasticsearchParser()
queries = parser.parse_file('SearchService.java', code)

context = AnalysisContext(
    file_path='SearchService.java',
    code_content=code
)

# å•é¡Œã‚’æ¤œå‡º
detector = WildcardDetector()
issues = detector.detect(queries, context)

# LLMã§è©³ç´°åˆ†æ
for issue in issues:
    print(f"\nAnalyzing: {issue.title}")

    # ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’æŠ½å‡ºï¼ˆå•é¡Œã®å‰å¾Œ5è¡Œï¼‰
    lines = code.split('\n')
    start = max(0, issue.line_number - 5)
    end = min(len(lines), issue.line_number + 5)
    code_snippet = '\n'.join(lines[start:end])

    # LLMã§æœ€é©åŒ–ææ¡ˆã‚’ç”Ÿæˆ
    result = optimizer.optimize_issue(
        issue=issue,
        code=code_snippet,
        language="java",
        db_type="elasticsearch"
    )

    print(f"\n=== Optimization Result ===")
    print(f"Root Cause: {result.root_cause}")
    print(f"Performance Impact: {result.performance_impact}")
    print(f"\nOptimized Code:\n{result.optimized_code}")
    print(f"\nImplementation Steps:")
    for i, step in enumerate(result.implementation_steps, 1):
        print(f"{i}. {step}")
    print(f"\nTesting Strategy: {result.testing_strategy}")
    print(f"Trade-offs: {result.trade_offs}")
    print(f"Confidence: {result.confidence_score:.2f}")

# APIä½¿ç”¨çµ±è¨ˆã‚’ç¢ºèª
stats = optimizer.get_usage_stats()
print(f"\n=== API Usage ===")
print(f"Total requests: {stats['total_requests']}")
print(f"Total cost: ${stats['total_cost_usd']:.4f}")
```

### ä¾‹6: ãƒãƒƒãƒæœ€é©åŒ–

```python
"""
è¤‡æ•°ã®å•é¡Œã‚’åŠ¹ç‡çš„ã«ãƒãƒƒãƒå‡¦ç†
"""
from multidb_analyzer.llm import LLMOptimizer
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import get_all_detectors
from multidb_analyzer.core.base_detector import AnalysisContext

def extract_code_snippets(code: str, issues: list, context_lines: int = 5):
    """å•é¡Œç®‡æ‰€ã®ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’æŠ½å‡º"""
    lines = code.split('\n')
    snippets = {}

    for issue in issues:
        start = max(0, issue.line_number - context_lines)
        end = min(len(lines), issue.line_number + context_lines)
        snippet = '\n'.join(lines[start:end])

        key = f"{issue.file_path}:{issue.line_number}"
        snippets[key] = snippet

    return snippets

# åˆ†æã¨ãƒãƒƒãƒæœ€é©åŒ–
optimizer = LLMOptimizer(api_key="sk-ant-xxxxx")
parser = JavaElasticsearchParser()
detectors = get_all_detectors()

with open('SearchService.java', 'r') as f:
    code = f.read()

queries = parser.parse_file('SearchService.java', code)
context = AnalysisContext(file_path='SearchService.java', code_content=code)

# ã™ã¹ã¦ã®å•é¡Œã‚’æ¤œå‡º
all_issues = []
for detector in detectors:
    issues = detector.detect(queries, context)
    all_issues.extend(issues)

print(f"Found {len(all_issues)} issues")

# ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’æŠ½å‡º
code_snippets = extract_code_snippets(code, all_issues)

# ãƒãƒƒãƒæœ€é©åŒ–ï¼ˆAPIã‚³ãƒ¼ãƒ«ã‚’æœ€å°åŒ–ï¼‰
print("Running batch optimization...")
results = optimizer.optimize_batch(
    issues=all_issues,
    code_snippets=code_snippets,
    language="java"
)

# çµæœã‚’ä¿å­˜
import json
output_data = {
    'total_issues': len(all_issues),
    'optimizations': [result.to_dict() for result in results]
}

with open('optimization_results.json', 'w') as f:
    json.dump(output_data, f, indent=2)

print(f"Saved results to optimization_results.json")
```

### ä¾‹7: å•é¡Œã®å„ªå…ˆé †ä½ä»˜ã‘

```python
"""
LLMã‚’ä½¿ã£ã¦å•é¡Œã®å„ªå…ˆé †ä½ã‚’æ±ºå®š
"""
from multidb_analyzer.llm import LLMOptimizer
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import get_all_detectors
from multidb_analyzer.core.base_detector import AnalysisContext

# å•é¡Œã‚’æ¤œå‡ºï¼ˆçœç•¥ï¼‰
# all_issues = ...

optimizer = LLMOptimizer()

# LLMã§å„ªå…ˆé †ä½ã‚’æ±ºå®š
print("Prioritizing issues with LLM...")
prioritization = optimizer.prioritize_issues(all_issues)

# Quick Winsã‚’è¡¨ç¤º
print("\n=== Quick Wins (easy, high impact) ===")
for issue_id in prioritization['quick_wins']:
    issue = all_issues[int(issue_id)]
    print(f"- {issue.title} ({issue.file_path}:{issue.line_number})")

# High Risk, High Rewardã‚’è¡¨ç¤º
print("\n=== High Risk, High Reward ===")
for issue_id in prioritization['high_risk_high_reward']:
    issue = all_issues[int(issue_id)]
    print(f"- {issue.title} ({issue.file_path}:{issue.line_number})")

# å„ªå…ˆé †ä½ä»˜ããƒªã‚¹ãƒˆã‚’è¡¨ç¤º
print("\n=== Prioritized Issues ===")
for item in prioritization['prioritized_issues']:
    issue = all_issues[item['issue_id']]
    print(f"{item['recommended_order']}. [{item['priority_score']:.2f}] {issue.title}")
    print(f"   {issue.file_path}:{issue.line_number}")
```

### ä¾‹8: è‡ªå‹•ä¿®æ­£ã®ç”Ÿæˆã¨é©ç”¨

```python
"""
è‡ªå‹•ä¿®æ­£ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆã¨é©ç”¨
"""
from multidb_analyzer.llm import LLMOptimizer
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import WildcardDetector
from multidb_analyzer.core.base_detector import AnalysisContext
import re

def apply_fix(file_path: str, line_number: int, original_code: str, fixed_code: str):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿®æ­£ã‚’é©ç”¨"""
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # å…ƒã®ã‚³ãƒ¼ãƒ‰ã‚’æ¤œç´¢ã—ã¦ç½®æ›
    for i, line in enumerate(lines):
        if original_code.strip() in line and i + 1 == line_number:
            lines[i] = line.replace(original_code.strip(), fixed_code.strip())
            break

    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
    backup_path = f"{file_path}.backup"
    with open(backup_path, 'w', encoding='utf-8') as f:
        f.writelines(lines)

    # ä¿®æ­£ã‚’é©ç”¨
    with open(file_path, 'w', encoding='utf-8') as f:
        f.writelines(lines)

    print(f"Applied fix to {file_path} (backup: {backup_path})")

# å•é¡Œã‚’æ¤œå‡º
optimizer = LLMOptimizer()
parser = JavaElasticsearchParser()

with open('SearchService.java', 'r') as f:
    code = f.read()

queries = parser.parse_file('SearchService.java', code)
context = AnalysisContext(file_path='SearchService.java', code_content=code)

detector = WildcardDetector()
issues = detector.detect(queries, context)

# è‡ªå‹•ä¿®æ­£ã‚’ç”Ÿæˆ
for issue in issues:
    print(f"\nGenerating fix for: {issue.title}")

    # ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’æŠ½å‡º
    lines = code.split('\n')
    problem_line = lines[issue.line_number - 1]

    # è‡ªå‹•ä¿®æ­£ã‚’ç”Ÿæˆ
    fix = optimizer.generate_auto_fix(
        issue=issue,
        code=problem_line,
        db_type="elasticsearch",
        framework="Spring Data",
        language="java"
    )

    print(f"Confidence: {fix['confidence']:.2f}")

    if fix['confidence'] > 0.8:
        print(f"Original: {problem_line.strip()}")
        print(f"Fixed:    {fix['fixed_code'].strip()}")

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª
        response = input("Apply this fix? (y/n): ")
        if response.lower() == 'y':
            apply_fix(
                file_path='SearchService.java',
                line_number=issue.line_number,
                original_code=problem_line.strip(),
                fixed_code=fix['fixed_code'].strip()
            )
        else:
            print("Skipped")
    else:
        print("Confidence too low, skipping auto-fix")
        print(f"Migration notes: {fix['migration_notes']}")
```

---

## ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### ä¾‹9: HTMLãƒ¬ãƒãƒ¼ãƒˆã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

```python
"""
ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã•ã‚ŒãŸHTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
"""
from multidb_analyzer.reporters import HTMLReporter, ReportConfig
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import get_all_detectors
from multidb_analyzer.core.base_detector import AnalysisContext
from datetime import datetime

# åˆ†æã‚’å®Ÿè¡Œï¼ˆçœç•¥ï¼‰
# all_issues = ...

# ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š
config = ReportConfig(
    title=f"Elasticsearch Analysis Report - {datetime.now().strftime('%Y-%m-%d')}",
    subtitle="SearchService.java Analysis",
    include_statistics=True,
    include_code_snippets=True,
    max_snippet_lines=10,
    group_by='severity',  # 'file', 'detector', 'severity'
    custom_css="""
        .critical { background-color: #ff4444; }
        .high { background-color: #ffaa00; }
        .medium { background-color: #ffdd00; }
    """
)

# HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
reporter = HTMLReporter(config=config)
reporter.generate(
    issues=all_issues,
    output_path='./reports/analysis_report.html'
)

print("Report generated: ./reports/analysis_report.html")
```

### ä¾‹10: è¤‡æ•°å½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```python
"""
HTMLã€JSONã€Markdownã‚’åŒæ™‚ã«ç”Ÿæˆ
"""
from multidb_analyzer.reporters import (
    HTMLReporter,
    JSONReporter,
    MarkdownReporter,
    ReportConfig
)

# åˆ†æçµæœï¼ˆçœç•¥ï¼‰
# all_issues = ...

output_dir = './reports'
base_config = ReportConfig(
    title="Elasticsearch Analysis Report",
    include_statistics=True
)

# HTML
html_reporter = HTMLReporter(base_config)
html_reporter.generate(all_issues, f"{output_dir}/report.html")

# JSON (CI/CDå‘ã‘)
json_reporter = JSONReporter(base_config)
json_reporter.generate(all_issues, f"{output_dir}/report.json")

# Markdown (GitHubå‘ã‘)
md_reporter = MarkdownReporter(base_config)
md_reporter.generate(all_issues, f"{output_dir}/REPORT.md")

print(f"Reports generated in {output_dir}/")
```

---

## CI/CDçµ±åˆ

### ä¾‹11: GitHub Actionsçµ±åˆ

```yaml
# .github/workflows/code-analysis.yml
name: Elasticsearch Code Analysis

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  analyze:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          cd phase4_multidb
          pip install -r requirements.txt
          pip install -e .

      - name: Run Elasticsearch Analysis
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python -m multidb_analyzer analyze ./src \
            --format json \
            --output ./reports/analysis.json

      - name: Check for critical issues
        run: |
          python .github/scripts/check_critical_issues.py ./reports/analysis.json

      - name: Upload report
        uses: actions/upload-artifact@v3
        with:
          name: analysis-report
          path: ./reports/analysis.json

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('./reports/analysis.json'));
            const body = `## Elasticsearch Analysis Results

            - Total Issues: ${report.total_issues}
            - Critical: ${report.critical_count}
            - High: ${report.high_count}

            [View Full Report](link-to-artifact)`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
```

**check_critical_issues.py**:
```python
#!/usr/bin/env python3
"""CI/CDã§ä½¿ç”¨: CRITICALãªå•é¡ŒãŒã‚ã‚‹å ´åˆã¯ãƒ“ãƒ«ãƒ‰å¤±æ•—"""
import sys
import json

def main():
    if len(sys.argv) < 2:
        print("Usage: check_critical_issues.py <report.json>")
        sys.exit(1)

    with open(sys.argv[1], 'r') as f:
        report = json.load(f)

    critical_issues = [
        issue for issue in report['issues']
        if issue['severity'] == 'CRITICAL'
    ]

    if critical_issues:
        print(f"âŒ Found {len(critical_issues)} CRITICAL issues:")
        for issue in critical_issues:
            print(f"  - {issue['title']} ({issue['file_path']}:{issue['line_number']})")
        sys.exit(1)
    else:
        print("âœ… No critical issues found")
        sys.exit(0)

if __name__ == '__main__':
    main()
```

---

## é«˜åº¦ãªä½¿ç”¨ä¾‹

### ä¾‹12: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

```python
"""
å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ä¸¦åˆ—åˆ†æ
"""
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import get_all_detectors
from multidb_analyzer.core.base_detector import AnalysisContext

def analyze_file(file_path: str):
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
    try:
        parser = JavaElasticsearchParser()
        detectors = get_all_detectors()

        with open(file_path, 'r', encoding='utf-8') as f:
            code = f.read()

        queries = parser.parse_file(file_path, code)
        if not queries:
            return []

        context = AnalysisContext(file_path=file_path, code_content=code)

        all_issues = []
        for detector in detectors:
            issues = detector.detect(queries, context)
            all_issues.extend(issues)

        return all_issues

    except Exception as e:
        print(f"Error analyzing {file_path}: {e}")
        return []

def parallel_analysis(project_dir: str, max_workers: int = 4):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä¸¦åˆ—åˆ†æ"""
    # Javaãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    project_path = Path(project_dir)
    java_files = list(project_path.rglob('*.java'))

    print(f"Analyzing {len(java_files)} files with {max_workers} workers...")

    all_issues = []
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¦åˆ—å‡¦ç†
        future_to_file = {
            executor.submit(analyze_file, str(f)): f
            for f in java_files
        }

        for future in as_completed(future_to_file):
            file_path = future_to_file[future]
            try:
                issues = future.result()
                all_issues.extend(issues)
                print(f"âœ“ {file_path.name}: {len(issues)} issues")
            except Exception as e:
                print(f"âœ— {file_path.name}: {e}")

    return all_issues

# å®Ÿè¡Œ
if __name__ == '__main__':
    issues = parallel_analysis('/path/to/large/project', max_workers=8)
    print(f"\nTotal: {len(issues)} issues found")
```

### ä¾‹13: ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

```python
"""
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å¤‰æ›´ã‚’ç›£è¦–ã—ã¦è‡ªå‹•åˆ†æ
"""
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from pathlib import Path
from multidb_analyzer.elasticsearch.parsers import JavaElasticsearchParser
from multidb_analyzer.elasticsearch.detectors import get_all_detectors
from multidb_analyzer.core.base_detector import AnalysisContext

class JavaFileHandler(FileSystemEventHandler):
    """Javaãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã‚’ç›£è¦–"""

    def __init__(self):
        self.parser = JavaElasticsearchParser()
        self.detectors = get_all_detectors()

    def on_modified(self, event):
        if event.is_directory:
            return

        if not event.src_path.endswith('.java'):
            return

        print(f"\nğŸ“ File modified: {event.src_path}")
        self.analyze_file(event.src_path)

    def analyze_file(self, file_path: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()

            queries = self.parser.parse_file(file_path, code)

            if not queries:
                print("  â„¹ï¸ No queries found")
                return

            context = AnalysisContext(file_path=file_path, code_content=code)

            all_issues = []
            for detector in self.detectors:
                issues = detector.detect(queries, context)
                all_issues.extend(issues)

            if all_issues:
                print(f"  âš ï¸ Found {len(all_issues)} issues:")
                for issue in all_issues[:5]:  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
                    print(f"    - {issue.severity.value}: {issue.title}")
                if len(all_issues) > 5:
                    print(f"    ... and {len(all_issues) - 5} more")
            else:
                print("  âœ… No issues found")

        except Exception as e:
            print(f"  âŒ Error: {e}")

# ç›£è¦–é–‹å§‹
if __name__ == '__main__':
    path = '/path/to/project/src'
    event_handler = JavaFileHandler()
    observer = Observer()
    observer.schedule(event_handler, path, recursive=True)
    observer.start()

    print(f"ğŸ‘€ Watching for changes in {path}...")
    print("Press Ctrl+C to stop")

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()

    observer.join()
```

---

## ã¾ã¨ã‚

ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰é›†ã§ã¯ã€ä»¥ä¸‹ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚«ãƒãƒ¼ã—ã¾ã—ãŸ:

1. **åŸºæœ¬çš„ãªä½¿ã„æ–¹**: ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†æã‹ã‚‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå…¨ä½“ã®åˆ†æã¾ã§
2. **ã‚«ã‚¹ã‚¿ãƒ æ¤œå‡ºå™¨**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ãƒ«ãƒ¼ãƒ«ã‚’å®Ÿè£…
3. **LLMçµ±åˆ**: Claude APIã‚’ä½¿ã£ãŸé«˜åº¦ãªåˆ†æ
4. **ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**: è¤‡æ•°å½¢å¼ã§ã®ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
5. **CI/CDçµ±åˆ**: GitHub Actionsã§ã®è‡ªå‹•åŒ–
6. **é«˜åº¦ãªä½¿ç”¨ä¾‹**: ä¸¦åˆ—å‡¦ç†ã¨ç¶™ç¶šçš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: [LLM_INTEGRATION.md](./LLM_INTEGRATION.md)ã§LLMçµ±åˆã®è©³ç´°ã‚’å­¦ã‚“ã§ãã ã•ã„ã€‚
